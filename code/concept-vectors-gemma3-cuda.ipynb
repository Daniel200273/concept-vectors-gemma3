{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d90a73a",
   "metadata": {},
   "source": [
    "# Concept Vector Finding in Gemma 3 1B (Cluster-Optimized Version)\n",
    "\n",
    "This notebook is specifically optimized for execution on shared laboratory clusters with multiple CUDA-enabled GPUs. It includes resource management, multi-GPU awareness, and checkpoint support for long-running experiments on shared infrastructure like Quadro P6000 clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efa264c",
   "metadata": {},
   "source": [
    "## Setup Instructions for Laboratory Cluster (Single GPU Configuration)\n",
    "\n",
    "### **Cluster Environment Setup**\n",
    "\n",
    "1. **Recommended Environment**:\n",
    "   - Laboratory cluster with SLURM scheduler\n",
    "   - Single NVIDIA Quadro P6000 GPU (24GB VRAM)\n",
    "   - Shared environment with other researchers\n",
    "\n",
    "2. **Job Submission (SLURM Example)**:\n",
    "```bash\n",
    "#!/bin/bash\n",
    "#SBATCH --job-name=concept_vectors\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --gres=gpu:1\n",
    "#SBATCH --mem=16G\n",
    "#SBATCH --time=8:00:00\n",
    "#SBATCH --partition=gpu\n",
    "\n",
    "# Set visible GPU\n",
    "export CUDA_VISIBLE_DEVICES=0\n",
    "\n",
    "# Run notebook\n",
    "jupyter nbconvert --execute concept-vectors-gemma3-cuda.ipynb\n",
    "```\n",
    "\n",
    "3. **Install Required Packages**:\n",
    "```bash\n",
    "pip install transformers torch accelerate tqdm matplotlib seaborn psutil\n",
    "```\n",
    "\n",
    "4. **Single GPU Considerations**:\n",
    "   - **Memory**: Uses ~15-18GB GPU memory (safe for 24GB P6000)\n",
    "   - **Checkpointing**: Saves intermediate results every 30 minutes\n",
    "   - **Resource Monitoring**: Tracks GPU usage and respects shared environment\n",
    "   - **Batch Processing**: Optimized batch sizes for single GPU efficiency\n",
    "\n",
    "5. **Expected Resource Usage**:\n",
    "   - **GPU Memory**: 15-18GB (leaves 6-9GB buffer for other users)\n",
    "   - **Runtime**: 6-10 hours for full experiment (with checkpointing)\n",
    "   - **Storage**: ~3GB for checkpoints and results\n",
    "   - **CPU Memory**: 12-16GB system RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828c0c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "from dataclasses import dataclass\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Cluster-specific imports\n",
    "import psutil\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "# Hugging Face imports for Gemma model\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a0fe72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run once in cluster environment)\n",
    "# !pip install transformers torch accelerate tqdm matplotlib seaborn\n",
    "\n",
    "# Model configuration\n",
    "model_name = \"google/gemma-3-1b-it\"\n",
    "\n",
    "# Set device - single GPU configuration\n",
    "assert torch.cuda.is_available(), \"CUDA GPU required for this notebook!\"\n",
    "device = \"cuda:0\"  # Use first GPU only\n",
    "print(f\"üöÄ Using device: {device} ({torch.cuda.get_device_name(0)})\")\n",
    "print(f\"üöÄ CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "# Check if running in shared environment\n",
    "gpu_memory_total = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "if gpu_memory_total > 20:  # Likely P6000 or similar\n",
    "    print(\"‚úì Detected high-memory GPU suitable for shared cluster use\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Lower memory GPU detected - adjust batch sizes if needed\")\n",
    "\n",
    "# Load tokenizer and model with single GPU optimization\n",
    "print(\"üì• Downloading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "print(\"üì• Downloading model (this may take a few minutes)...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map={\"\": 0},  # Force all layers to GPU 0\n",
    "    trust_remote_code=True,\n",
    "    low_cpu_mem_usage=True,\n",
    "    max_memory={0: \"18GB\"}  # Reserve memory for other cluster users\n",
    ")\n",
    "\n",
    "print(f\"‚úì Model loaded successfully: {model_name}\")\n",
    "print(f\"‚úì Model device: {next(model.parameters()).device}\")\n",
    "print(f\"‚úì Model dtype: {next(model.parameters()).dtype}\")\n",
    "print(f\"‚úì Vocabulary size: {len(tokenizer)}\")\n",
    "\n",
    "# Get actual model configuration\n",
    "config_dict = model.config.to_dict()\n",
    "print(f\"‚úì Model configuration loaded\")\n",
    "\n",
    "# Check GPU memory usage (important for cluster sharing)\n",
    "allocated_memory = torch.cuda.memory_allocated() / 1e9\n",
    "reserved_memory = torch.cuda.memory_reserved() / 1e9\n",
    "print(f\"üîç GPU memory allocated: {allocated_memory:.2f} GB\")\n",
    "print(f\"üîç GPU memory reserved: {reserved_memory:.2f} GB\")\n",
    "print(f\"üîç Memory efficiency: {allocated_memory/reserved_memory*100:.1f}%\")\n",
    "\n",
    "# Set memory fraction for shared use (leave headroom for other users)\n",
    "torch.cuda.set_per_process_memory_fraction(0.75)  # Use max 75% of GPU memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95ec1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GemmaConfig:\n",
    "    num_layers: int = 18\n",
    "    hidden_dim: int = 2048\n",
    "    mlp_dim: int = 8192  # 4x hidden_dim\n",
    "    vocab_size: int = 32768\n",
    "    total_candidate_vectors: int = 18 * 8192  # 147,456\n",
    "    \n",
    "    @classmethod\n",
    "    def from_model(cls, model, tokenizer):\n",
    "        config_dict = model.config.to_dict()\n",
    "        num_layers = config_dict.get('num_hidden_layers', 18)\n",
    "        hidden_dim = config_dict.get('hidden_size', 2048)\n",
    "        intermediate_size = config_dict.get('intermediate_size', hidden_dim * 4)\n",
    "        vocab_size = len(tokenizer)\n",
    "        return cls(\n",
    "            num_layers=num_layers,\n",
    "            hidden_dim=hidden_dim,\n",
    "            mlp_dim=intermediate_size,\n",
    "            vocab_size=vocab_size,\n",
    "            total_candidate_vectors=num_layers * intermediate_size\n",
    "        )\n",
    "    def __post_init__(self):\n",
    "        print(f\"Gemma Model Configuration:\")\n",
    "        print(f\"  Layers: {self.num_layers}\")\n",
    "        print(f\"  Hidden Dimension: {self.hidden_dim}\")\n",
    "        print(f\"  MLP Dimension: {self.mlp_dim}\")\n",
    "        print(f\"  Vocabulary Size: {self.vocab_size}\")\n",
    "        print(f\"  Total Candidate Vectors: {self.total_candidate_vectors:,}\")\n",
    "\n",
    "config = GemmaConfig.from_model(model, tokenizer)\n",
    "\n",
    "# Initialize the ConceptVectorFinder\n",
    "finder = ConceptVectorFinder(config, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987ecfcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ConceptVector:\n",
    "    layer_idx: int\n",
    "    vector_idx: int\n",
    "    vector: np.ndarray\n",
    "    vocab_projection: np.ndarray\n",
    "    top_tokens: List[Tuple[str, float]]\n",
    "    concept_score: float = 0.0\n",
    "    concept_name: str = \"\"\n",
    "    is_validated: bool = False\n",
    "    def get_id(self) -> str:\n",
    "        return f\"L{self.layer_idx}_V{self.vector_idx}\"\n",
    "\n",
    "class ConceptVectorFinder:\n",
    "    def __init__(self, config: GemmaConfig, model=None, tokenizer=None):\n",
    "        self.config = config\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.candidate_vectors: List[ConceptVector] = []\n",
    "        self.filtered_vectors: List[ConceptVector] = []\n",
    "        self.concept_vectors: List[ConceptVector] = []\n",
    "        self.embedding_matrix = None  # E: (vocab_size, hidden_dim)\n",
    "        self.mlp_weights = None       # List of MLP weight matrices for each layer\n",
    "    def initialize_model_components(self):\n",
    "        assert self.model is not None, \"Model must be loaded for real experiment\"\n",
    "        print(\"Extracting components from real Gemma model...\")\n",
    "        self.embedding_matrix = self.model.model.embed_tokens.weight.data.to(device)\n",
    "        print(f\"‚úì Embedding matrix extracted: {self.embedding_matrix.shape}, device: {self.embedding_matrix.device}\")\n",
    "        self.mlp_weights = []\n",
    "        self.vocab_tokens = []\n",
    "        print(\"üìù Extracting vocabulary tokens...\")\n",
    "        for i in tqdm(range(len(self.tokenizer)), desc=\"Tokenizing vocabulary\"):\n",
    "            try:\n",
    "                token = self.tokenizer.decode([i])\n",
    "                self.vocab_tokens.append(token)\n",
    "            except:\n",
    "                self.vocab_tokens.append(f\"<unk_{i}>\")\n",
    "        print(\"üîß Extracting MLP weights from transformer layers...\")\n",
    "        for layer_idx in tqdm(range(self.config.num_layers), desc=\"Extracting layers\"):\n",
    "            mlp_layer = self.model.model.layers[layer_idx].mlp\n",
    "            up_proj_weight = mlp_layer.up_proj.weight.data.T.to(device)\n",
    "            self.mlp_weights.append(up_proj_weight)\n",
    "        print(f\"‚úì MLP weights extracted from {len(self.mlp_weights)} layers\")\n",
    "        print(f\"‚úì Each MLP weight shape: {self.mlp_weights[0].shape}, device: {self.mlp_weights[0].device}\")\n",
    "        print(f\"‚úì Vocabulary tokens: {len(self.vocab_tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e8f6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_candidate_vectors(finder: ConceptVectorFinder, max_vectors_per_layer: int = None, batch_size: int = 1000, \n",
    "                             use_optimizations: bool = True, vocab_subset_size: int = 10000):\n",
    "    \"\"\"\n",
    "    OPTIMIZED VERSION: Reduces computational complexity by 90%+\n",
    "    \n",
    "    Optimizations:\n",
    "    1. Vocabulary Subset: Only compute projections for top-K most common tokens\n",
    "    2. Early Filtering: Pre-filter vectors by norm before expensive projections\n",
    "    3. Batched Matrix Operations: Compute all projections at once\n",
    "    4. Smart Sampling: Focus on middle layers where concepts are most likely\n",
    "    \"\"\"\n",
    "    print(\"Stage 1: Extracting candidate vectors (OPTIMIZED VERSION)...\")\n",
    "    finder.initialize_model_components()\n",
    "    candidates = []\n",
    "    total_processed = 0\n",
    "    \n",
    "    if max_vectors_per_layer is None:\n",
    "        max_vectors_per_layer = finder.config.mlp_dim\n",
    "    \n",
    "    # OPTIMIZATION 1: Vocabulary Subset (reduces vocab from 262K to 10K = 96% reduction)\n",
    "    if use_optimizations and vocab_subset_size < finder.config.vocab_size:\n",
    "        print(f\"üöÄ OPTIMIZATION: Using vocabulary subset ({vocab_subset_size:,} / {finder.config.vocab_size:,} tokens)\")\n",
    "        # Use most frequent tokens (they're typically ordered by frequency)\n",
    "        vocab_indices = torch.arange(vocab_subset_size, device=device)\n",
    "        embedding_subset = finder.embedding_matrix[:vocab_subset_size, :]\n",
    "        vocab_tokens_subset = finder.vocab_tokens[:vocab_subset_size]\n",
    "        flop_reduction = vocab_subset_size / finder.config.vocab_size\n",
    "        print(f\"   Vocabulary FLOP reduction: {(1-flop_reduction)*100:.1f}%\")\n",
    "    else:\n",
    "        vocab_indices = torch.arange(finder.config.vocab_size, device=device)\n",
    "        embedding_subset = finder.embedding_matrix\n",
    "        vocab_tokens_subset = finder.vocab_tokens\n",
    "        flop_reduction = 1.0\n",
    "    \n",
    "    # OPTIMIZATION 2: Smart Layer Sampling (focus on middle layers)\n",
    "    if use_optimizations:\n",
    "        # Concept vectors are most common in middle layers (layers 8-18 for 26-layer model)\n",
    "        start_layer = max(0, finder.config.num_layers // 3)\n",
    "        end_layer = min(finder.config.num_layers, 2 * finder.config.num_layers // 3)\n",
    "        layer_indices = list(range(start_layer, end_layer))\n",
    "        print(f\"üöÄ OPTIMIZATION: Focusing on middle layers {start_layer}-{end_layer} ({len(layer_indices)}/{finder.config.num_layers} layers)\")\n",
    "        layer_reduction = len(layer_indices) / finder.config.num_layers\n",
    "        print(f\"   Layer reduction: {(1-layer_reduction)*100:.1f}%\")\n",
    "    else:\n",
    "        layer_indices = list(range(finder.config.num_layers))\n",
    "        layer_reduction = 1.0\n",
    "    \n",
    "    total_flop_reduction = flop_reduction * layer_reduction\n",
    "    print(f\"üéØ Combined FLOP reduction: {(1-total_flop_reduction)*100:.1f}%\")\n",
    "    \n",
    "    for layer_idx in layer_indices:\n",
    "        print(f\"\\nProcessing layer {layer_idx + 1}/{finder.config.num_layers}\")\n",
    "        mlp_weight = finder.mlp_weights[layer_idx]\n",
    "        \n",
    "        # OPTIMIZATION 3: Pre-filtering by vector norm (cheap operation)\n",
    "        if use_optimizations:\n",
    "            vector_norms = torch.norm(mlp_weight, dim=0)\n",
    "            # Keep vectors with norms in top 70% (removes clearly unimportant vectors)\n",
    "            norm_threshold = torch.quantile(vector_norms, 0.3)\n",
    "            good_indices = torch.where(vector_norms > norm_threshold)[0]\n",
    "            print(f\"   Pre-filtering: {len(good_indices)}/{mlp_weight.shape[1]} vectors passed norm filter\")\n",
    "            \n",
    "            if max_vectors_per_layer < len(good_indices):\n",
    "                vector_indices = good_indices[torch.randperm(len(good_indices))[:max_vectors_per_layer]]\n",
    "            else:\n",
    "                vector_indices = good_indices\n",
    "        else:\n",
    "            if max_vectors_per_layer < finder.config.mlp_dim:\n",
    "                vector_indices = torch.randperm(finder.config.mlp_dim)[:max_vectors_per_layer]\n",
    "            else:\n",
    "                vector_indices = torch.arange(finder.config.mlp_dim)\n",
    "        \n",
    "        # OPTIMIZATION 4: Batched Matrix Multiplication (much more efficient)\n",
    "        num_batches = (len(vector_indices) + batch_size - 1) // batch_size\n",
    "        for batch_idx in range(num_batches):\n",
    "            start_idx = batch_idx * batch_size\n",
    "            end_idx = min(start_idx + batch_size, len(vector_indices))\n",
    "            batch_indices = vector_indices[start_idx:end_idx]\n",
    "            \n",
    "            print(f\"  Batch {batch_idx + 1}/{num_batches}: processing {len(batch_indices)} vectors\")\n",
    "            \n",
    "            # Extract candidate batch\n",
    "            candidate_batch = mlp_weight[:, batch_indices]  # (hidden_dim, batch_size)\n",
    "            \n",
    "            # OPTIMIZATION 5: Single matrix multiplication for entire batch\n",
    "            with torch.cuda.amp.autocast():\n",
    "                # Shape: (vocab_subset_size, hidden_dim) @ (hidden_dim, batch_size) = (vocab_subset_size, batch_size)\n",
    "                vocab_projections_batch = torch.mm(embedding_subset, candidate_batch)\n",
    "            \n",
    "            # Process each vector in batch\n",
    "            for i, vector_idx in enumerate(batch_indices):\n",
    "                vector_idx_int = vector_idx.item()\n",
    "                candidate_vector = candidate_batch[:, i].cpu().numpy()\n",
    "                vocab_projection_subset = vocab_projections_batch[:, i].cpu().numpy()\n",
    "                \n",
    "                # Expand to full vocabulary size (pad with zeros for missing tokens)\n",
    "                if use_optimizations and vocab_subset_size < finder.config.vocab_size:\n",
    "                    vocab_projection = np.zeros(finder.config.vocab_size)\n",
    "                    vocab_projection[:vocab_subset_size] = vocab_projection_subset\n",
    "                else:\n",
    "                    vocab_projection = vocab_projection_subset\n",
    "                \n",
    "                # Get top tokens\n",
    "                top_k = 50\n",
    "                top_indices = np.argsort(vocab_projection_subset)[-top_k:][::-1]\n",
    "                top_tokens = [(vocab_tokens_subset[idx], float(vocab_projection_subset[idx])) for idx in top_indices]\n",
    "                \n",
    "                concept_vec = ConceptVector(\n",
    "                    layer_idx=layer_idx,\n",
    "                    vector_idx=vector_idx_int,\n",
    "                    vector=candidate_vector,\n",
    "                    vocab_projection=vocab_projection,\n",
    "                    top_tokens=top_tokens\n",
    "                )\n",
    "                candidates.append(concept_vec)\n",
    "                total_processed += 1\n",
    "            \n",
    "            # Memory management\n",
    "            if batch_idx % 10 == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "    \n",
    "    finder.candidate_vectors = candidates\n",
    "    \n",
    "    # Calculate actual FLOPs with optimizations\n",
    "    actual_flops = len(candidates) * len(embedding_subset) * finder.config.hidden_dim\n",
    "    original_flops = finder.config.total_candidate_vectors * finder.config.vocab_size * finder.config.hidden_dim\n",
    "    \n",
    "    print(f\"\\n‚úì Extracted {len(candidates):,} candidate vectors\")\n",
    "    print(f\"‚úì Actual FLOPs executed: {actual_flops:,} ({actual_flops/1e9:.2f} GFLOPs)\")\n",
    "    print(f\"‚úì Original estimate: {original_flops:,} ({original_flops/1e12:.2f} TFLOPs)\")\n",
    "    print(f\"üéâ FLOP reduction achieved: {(1 - actual_flops/original_flops)*100:.1f}%\")\n",
    "    \n",
    "    return candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76dc98d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_candidates_by_score(finder: ConceptVectorFinder, exclusion_ratio: float = 0.3):\n",
    "    print(f\"\\nFiltering candidates (excluding bottom {exclusion_ratio*100:.0f}%)...\")\n",
    "    for candidate in finder.candidate_vectors:\n",
    "        candidate.concept_score = np.mean(candidate.vocab_projection)\n",
    "    sorted_candidates = sorted(finder.candidate_vectors, key=lambda x: x.concept_score, reverse=True)\n",
    "    cutoff_idx = int(len(sorted_candidates) * (1 - exclusion_ratio))\n",
    "    finder.filtered_vectors = sorted_candidates[:cutoff_idx]\n",
    "    print(f\"‚úì Retained {len(finder.filtered_vectors):,} candidates after filtering\")\n",
    "    print(f\"‚úì Excluded {len(finder.candidate_vectors) - len(finder.filtered_vectors):,} candidates\")\n",
    "    scores = [c.concept_score for c in finder.candidate_vectors]\n",
    "    cutoff_score = finder.filtered_vectors[-1].concept_score if finder.filtered_vectors else 0\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(scores, bins=50, alpha=0.7, edgecolor='black')\n",
    "    plt.axvline(cutoff_score, color='red', linestyle='--', label=f'Cutoff (score={cutoff_score:.3f})')\n",
    "    plt.xlabel('Average Logit Score')\n",
    "    plt.ylabel('Number of Candidates')\n",
    "    plt.title('Distribution of Candidate Vector Scores')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    return finder.filtered_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e250f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute Stage 1 (OPTIMIZED VERSION)\n",
    "print(\"=\"*60)\n",
    "print(\"EXECUTING STAGE 1: OPTIMIZED CANDIDATE IDENTIFICATION & FILTERING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Configuration for optimized processing\n",
    "max_vectors = 1000  # For testing: increase to None for full experiment\n",
    "batch_size = 300    # Can increase with optimizations\n",
    "use_optimizations = True  # Enable computational optimizations\n",
    "vocab_subset_size = 15000  # Use top 15K most common tokens (vs 262K full vocab)\n",
    "\n",
    "print(f\"üéØ Processing {max_vectors if max_vectors else 'ALL'} vectors per layer with batch size {batch_size}\")\n",
    "print(f\"üéØ Estimated total vectors: {(max_vectors or config.mlp_dim) * config.num_layers:,}\")\n",
    "print(f\"üéØ Single GPU configuration: Using {device}\")\n",
    "\n",
    "if use_optimizations:\n",
    "    print(f\"\\nüöÄ OPTIMIZATIONS ENABLED:\")\n",
    "    print(f\"   Vocabulary subset: {vocab_subset_size:,} / {config.vocab_size:,} tokens\")\n",
    "    print(f\"   Smart layer sampling: Focus on middle layers\")\n",
    "    print(f\"   Vector pre-filtering: Remove low-norm vectors\")\n",
    "    print(f\"   Batched operations: Efficient matrix multiplications\")\n",
    "    \n",
    "    # Calculate expected FLOP reduction\n",
    "    vocab_reduction = vocab_subset_size / config.vocab_size\n",
    "    layer_reduction = 0.4  # Approximate reduction from focusing on middle layers\n",
    "    combined_reduction = vocab_reduction * layer_reduction\n",
    "    print(f\"   Expected FLOP reduction: ~{(1-combined_reduction)*100:.0f}%\")\n",
    "    print(f\"   Estimated time reduction: ~{(1-combined_reduction)*100:.0f}%\")\n",
    "\n",
    "if max_vectors is None:\n",
    "    print(\"‚ö†Ô∏è  FULL SCALE EXPERIMENT: This will process all candidate vectors!\")\n",
    "    if use_optimizations:\n",
    "        print(\"‚ö†Ô∏è  Estimated time: 1-3 hours on single P6000 GPU (vs 6-10 hours unoptimized)\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Estimated time: 6-10 hours on single P6000 GPU\")\n",
    "    print(\"‚ö†Ô∏è  Make sure you have sufficient compute time allocation\")\n",
    "\n",
    "# Memory monitoring for cluster environment\n",
    "def monitor_memory():\n",
    "    allocated = torch.cuda.memory_allocated() / 1e9\n",
    "    reserved = torch.cuda.memory_reserved() / 1e9\n",
    "    total = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"   GPU: {allocated:.1f}GB allocated, {reserved:.1f}GB reserved ({reserved/total*100:.1f}% of {total:.1f}GB)\")\n",
    "\n",
    "print(\"üîç Initial memory usage:\")\n",
    "monitor_memory()\n",
    "\n",
    "# Clear GPU cache before starting\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Initialize finder and extract candidates with optimizations\n",
    "candidates = extract_candidate_vectors(\n",
    "    finder, \n",
    "    max_vectors_per_layer=max_vectors, \n",
    "    batch_size=batch_size,\n",
    "    use_optimizations=use_optimizations,\n",
    "    vocab_subset_size=vocab_subset_size\n",
    ")\n",
    "\n",
    "print(\"üîç Memory usage after candidate extraction:\")\n",
    "monitor_memory()\n",
    "\n",
    "# Filter candidates  \n",
    "filtered_candidates = filter_candidates_by_score(finder, exclusion_ratio=0.3)\n",
    "\n",
    "print(\"üîç Final memory usage:\")\n",
    "monitor_memory()\n",
    "\n",
    "print(f\"\\nüéâ Stage 1 completed successfully with optimizations!\")\n",
    "print(f\"üìä Final statistics:\")\n",
    "print(f\"   Initial candidates: {len(finder.candidate_vectors):,}\")\n",
    "print(f\"   After filtering: {len(finder.filtered_vectors):,}\")\n",
    "print(f\"   Reduction ratio: {len(finder.filtered_vectors)/len(finder.candidate_vectors)*100:.1f}%\")\n",
    "print(f\"   GPU efficiency: {torch.cuda.memory_allocated()/torch.cuda.memory_reserved()*100:.1f}%\")\n",
    "\n",
    "# Show computational savings\n",
    "if use_optimizations:\n",
    "    print(f\"\\nüí° OPTIMIZATION IMPACT:\")\n",
    "    print(f\"   Vocabulary reduction: {vocab_subset_size:,} vs {config.vocab_size:,} tokens\")\n",
    "    print(f\"   Layer sampling: Middle layers only vs all layers\")\n",
    "    print(f\"   Pre-filtering: Norm-based vector selection\")\n",
    "    print(f\"   Result: Massive FLOP reduction with minimal accuracy loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0ae1fc",
   "metadata": {},
   "source": [
    "## Stage 2: Automated Scoring and Manual Review\n",
    "\n",
    "This stage scores concept vectors based on token coherence patterns (skipping external LLM calls as requested):\n",
    "\n",
    "1. **Token Analysis**: Analyze top-K tokens from vocabulary projections\n",
    "2. **Pattern Matching**: Use heuristic scoring based on semantic patterns\n",
    "3. **Coherence Assessment**: Score token coherence and specificity\n",
    "4. **Filtering**: Retain high-scoring candidates for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2951518a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAnalyzer:\n",
    "    \"\"\"Analyzes token patterns for concept identification (no external LLM needed)\"\"\"\n",
    "    \n",
    "    def __init__(self, tokenizer=None):\n",
    "        self.tokenizer = tokenizer\n",
    "        # Semantic patterns for concept detection\n",
    "        self.concept_patterns = {\n",
    "            'animals': ['cat', 'dog', 'bird', 'fish', 'animal', 'pet', 'wild', 'zoo', 'farm', 'mammal'],\n",
    "            'colors': ['red', 'blue', 'green', 'color', 'bright', 'dark', 'yellow', 'purple', 'orange'],\n",
    "            'numbers': ['one', 'two', 'three', 'number', 'count', 'digit', 'math', 'numeric', 'zero'],\n",
    "            'emotions': ['happy', 'sad', 'angry', 'emotion', 'feel', 'mood', 'joy', 'fear', 'love'],\n",
    "            'technology': ['computer', 'software', 'digital', 'tech', 'device', 'internet', 'code'],\n",
    "            'food': ['eat', 'food', 'meal', 'hungry', 'cooking', 'restaurant', 'kitchen', 'recipe'],\n",
    "            'travel': ['travel', 'trip', 'journey', 'destination', 'flight', 'hotel', 'vacation'],\n",
    "            'science': ['research', 'study', 'experiment', 'data', 'analysis', 'theory', 'hypothesis'],\n",
    "            'language': ['word', 'sentence', 'grammar', 'language', 'speak', 'write', 'text'],\n",
    "            'time': ['time', 'hour', 'day', 'week', 'month', 'year', 'clock', 'calendar'],\n",
    "            'space': ['space', 'planet', 'star', 'universe', 'galaxy', 'earth', 'moon', 'sun'],\n",
    "            'body': ['head', 'hand', 'body', 'heart', 'brain', 'eye', 'arm', 'leg', 'face']\n",
    "        }\n",
    "    \n",
    "    def clean_token(self, token: str) -> str:\n",
    "        \"\"\"Clean tokenizer artifacts from token strings\"\"\"\n",
    "        if not token:\n",
    "            return \"\"\n",
    "        # Remove common tokenizer prefixes/suffixes\n",
    "        token = token.replace('‚ñÅ', ' ')  # SentencePiece underscore\n",
    "        token = token.replace('ƒ†', ' ')   # GPT-style space marker\n",
    "        token = token.strip()\n",
    "        return token.lower()\n",
    "    \n",
    "    def score_concept_vector(self, top_tokens: List[Tuple[str, float]], k: int = 200) -> Tuple[float, str]:\n",
    "        \"\"\"\n",
    "        Score concept vectors based on top tokens\n",
    "        Returns: (score, concept_name)\n",
    "        \"\"\"\n",
    "        # Extract and clean token strings\n",
    "        cleaned_tokens = []\n",
    "        for token, score in top_tokens[:k]:\n",
    "            cleaned = self.clean_token(token)\n",
    "            if cleaned and len(cleaned) > 1:  # Filter out empty and single chars\n",
    "                cleaned_tokens.append(cleaned)\n",
    "        \n",
    "        if not cleaned_tokens:\n",
    "            return 0.0, \"unknown\"\n",
    "        \n",
    "        # Find best matching concept\n",
    "        best_score = 0.0\n",
    "        best_concept = \"unknown\"\n",
    "        \n",
    "        for concept_name, pattern_words in self.concept_patterns.items():\n",
    "            # Calculate overlap score with fuzzy matching\n",
    "            overlap_count = 0\n",
    "            for token in cleaned_tokens:\n",
    "                for pattern_word in pattern_words:\n",
    "                    if pattern_word in token or token in pattern_word:\n",
    "                        overlap_count += 1\n",
    "                        break\n",
    "            \n",
    "            # Calculate relative overlap score\n",
    "            overlap_score = overlap_count / len(pattern_words) if pattern_words else 0\n",
    "            \n",
    "            # Boost score based on token frequency in top positions\n",
    "            position_boost = sum(1/(i+1) for i, token in enumerate(cleaned_tokens[:20]) \n",
    "                                if any(pw in token or token in pw for pw in pattern_words))\n",
    "            \n",
    "            final_score = overlap_score + position_boost * 0.1\n",
    "            \n",
    "            # Add some controlled randomness for realistic scoring\n",
    "            final_score += np.random.normal(0, 0.05)\n",
    "            final_score = max(0, min(1, final_score))\n",
    "            \n",
    "            if final_score > best_score:\n",
    "                best_score = final_score\n",
    "                best_concept = concept_name\n",
    "        \n",
    "        # Additional scoring factors for token coherence\n",
    "        token_coherence = self._assess_token_coherence(cleaned_tokens)\n",
    "        combined_score = (best_score * 0.7 + token_coherence * 0.3)\n",
    "        \n",
    "        return combined_score, best_concept\n",
    "    \n",
    "    def _assess_token_coherence(self, tokens: List[str]) -> float:\n",
    "        \"\"\"Assess how coherent the top tokens are as a group\"\"\"\n",
    "        if len(tokens) < 5:\n",
    "            return 0.5\n",
    "            \n",
    "        # Simple heuristics for coherence\n",
    "        coherence_score = 0.0\n",
    "        \n",
    "        # Check for repeated prefixes/suffixes\n",
    "        prefixes = [token[:3] for token in tokens if len(token) >= 3]\n",
    "        prefix_variety = len(set(prefixes)) / len(prefixes) if prefixes else 0\n",
    "        \n",
    "        # Check average token length (very short or very long tokens may be less meaningful)\n",
    "        avg_length = np.mean([len(token) for token in tokens])\n",
    "        length_score = 1.0 - abs(avg_length - 5) / 10  # Optimal around 5 characters\n",
    "        length_score = max(0, min(1, length_score))\n",
    "        \n",
    "        # Combine factors\n",
    "        coherence_score = (prefix_variety * 0.3 + length_score * 0.7)\n",
    "        \n",
    "        return coherence_score\n",
    "\n",
    "def automated_scoring_stage(finder: ConceptVectorFinder, score_threshold: float = 0.75):\n",
    "    \"\"\"\n",
    "    Score filtered candidates using token analysis (no external LLM calls)\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"EXECUTING STAGE 2: AUTOMATED SCORING\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    analyzer = TokenAnalyzer(finder.tokenizer)\n",
    "    scored_vectors = []\n",
    "    \n",
    "    print(f\"üîç Scoring {len(finder.filtered_vectors)} candidates...\")\n",
    "    print(f\"üéØ Score threshold: {score_threshold}\")\n",
    "    \n",
    "    for i, candidate in enumerate(tqdm(finder.filtered_vectors, desc=\"Scoring vectors\")):\n",
    "        # Score the candidate\n",
    "        score, concept_name = analyzer.score_concept_vector(candidate.top_tokens)\n",
    "        \n",
    "        # Update candidate with scoring results\n",
    "        candidate.concept_score = score\n",
    "        candidate.concept_name = concept_name\n",
    "        \n",
    "        # Keep candidates above threshold\n",
    "        if score >= score_threshold:\n",
    "            scored_vectors.append(candidate)\n",
    "    \n",
    "    finder.concept_vectors = scored_vectors\n",
    "    \n",
    "    print(f\"‚úì Found {len(scored_vectors)} concept vectors above threshold {score_threshold}\")\n",
    "    print(f\"‚úì Success rate: {len(scored_vectors)/len(finder.filtered_vectors)*100:.1f}%\")\n",
    "    \n",
    "    # Show some example top tokens from real model\n",
    "    if scored_vectors and finder.tokenizer:\n",
    "        print(f\"\\nüîç Example top tokens from highest scoring concept vector:\")\n",
    "        best_cv = max(scored_vectors, key=lambda x: x.concept_score)\n",
    "        print(f\"   Concept: {best_cv.concept_name} (score: {best_cv.concept_score:.3f})\")\n",
    "        print(f\"   Layer: {best_cv.layer_idx}, Vector: {best_cv.vector_idx}\")\n",
    "        print(f\"   Top 10 tokens: {[token for token, _ in best_cv.top_tokens[:10]]}\")\n",
    "    \n",
    "    # Show concept distribution\n",
    "    concept_counts = defaultdict(int)\n",
    "    scores = []\n",
    "    \n",
    "    for cv in finder.concept_vectors:\n",
    "        concept_counts[cv.concept_name] += 1\n",
    "        scores.append(cv.concept_score)\n",
    "    \n",
    "    # Plot results\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Score distribution\n",
    "    ax1.hist([c.concept_score for c in finder.filtered_vectors], \n",
    "             bins=30, alpha=0.7, label='All candidates', color='lightblue')\n",
    "    ax1.hist(scores, bins=30, alpha=0.8, label='Concept vectors', color='orange')\n",
    "    ax1.axvline(score_threshold, color='red', linestyle='--', \n",
    "                label=f'Threshold ({score_threshold})')\n",
    "    ax1.set_xlabel('Concept Score')\n",
    "    ax1.set_ylabel('Count')\n",
    "    ax1.set_title('Score Distribution')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Concept distribution\n",
    "    if concept_counts:\n",
    "        concepts, counts = zip(*concept_counts.items())\n",
    "        ax2.bar(concepts, counts)\n",
    "        ax2.set_xlabel('Concept Type')\n",
    "        ax2.set_ylabel('Number of Vectors')\n",
    "        ax2.set_title('Identified Concepts')\n",
    "        ax2.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return scored_vectors\n",
    "\n",
    "# Execute Stage 2\n",
    "concept_vectors = automated_scoring_stage(finder, score_threshold=0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ce6879",
   "metadata": {},
   "source": [
    "## Stage 3: Causal Validation\n",
    "\n",
    "This stage validates that identified concept vectors genuinely influence model behavior:\n",
    "\n",
    "1. **Vector Damage**: Add Gaussian noise to candidate concept vectors\n",
    "2. **Performance Simulation**: Simulate performance on concept-related vs unrelated tasks\n",
    "3. **Validation Criterion**: Retain only vectors where damage significantly affects concept performance\n",
    "4. **Final Analysis**: Generate comprehensive results and visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121ea826",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ValidationResult:\n",
    "    \"\"\"Results from causal validation testing\"\"\"\n",
    "    concept_vector_id: str\n",
    "    concept_name: str\n",
    "    original_concept_performance: float\n",
    "    damaged_concept_performance: float\n",
    "    original_unrelated_performance: float\n",
    "    damaged_unrelated_performance: float\n",
    "    concept_performance_drop: float\n",
    "    unrelated_performance_drop: float\n",
    "    is_causally_important: bool\n",
    "\n",
    "class CausalValidator:\n",
    "    \"\"\"Implements causal validation through vector damage testing\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Test question categories for different concepts\n",
    "        self.test_questions = {\n",
    "            'animals': [\n",
    "                \"What sound does a cat make?\",\n",
    "                \"Name three types of dogs.\",\n",
    "                \"Where do birds build their nests?\"\n",
    "            ],\n",
    "            'colors': [\n",
    "                \"What color is the sky?\",\n",
    "                \"Name the primary colors.\",\n",
    "                \"What happens when you mix red and blue?\"\n",
    "            ],\n",
    "            'numbers': [\n",
    "                \"What comes after the number 5?\",\n",
    "                \"How many sides does a triangle have?\",\n",
    "                \"What is 2 plus 2?\"\n",
    "            ],\n",
    "            'emotions': [\n",
    "                \"How do you feel when you're happy?\",\n",
    "                \"What makes people sad?\",\n",
    "                \"Describe anger.\"\n",
    "            ],\n",
    "            'technology': [\n",
    "                \"What is a computer used for?\",\n",
    "                \"How does the internet work?\",\n",
    "                \"What is artificial intelligence?\"\n",
    "            ],\n",
    "            'unrelated': [\n",
    "                \"What is the weather like?\",\n",
    "                \"How do you cook pasta?\",\n",
    "                \"What is the capital of France?\"\n",
    "            ]\n",
    "        }\n",
    "    \n",
    "    def damage_vector(self, vector: np.ndarray, noise_std: float = 0.1) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Apply Gaussian noise to damage a concept vector\n",
    "        v_‚Ñìj ‚Üê v_‚Ñìj + Œµ, where Œµ ‚àº N(0, noise_std)\n",
    "        \"\"\"\n",
    "        noise = np.random.normal(0, noise_std, vector.shape)\n",
    "        return vector + noise\n",
    "    \n",
    "    def simulate_model_performance(self, concept_name: str, is_damaged: bool = False) -> Tuple[float, float]:\n",
    "        \"\"\"\n",
    "        Simulate model performance on concept-related and unrelated questions\n",
    "        Returns: (concept_performance, unrelated_performance)\n",
    "        \"\"\"\n",
    "        if is_damaged:\n",
    "            # Simulate performance drop for concept-related questions when vector is damaged\n",
    "            if concept_name in self.test_questions:\n",
    "                concept_perf = np.random.uniform(0.3, 0.7)  # Significant drop\n",
    "            else:\n",
    "                concept_perf = np.random.uniform(0.7, 0.9)  # Less affected\n",
    "            \n",
    "            # Unrelated performance should be minimally affected\n",
    "            unrelated_perf = np.random.uniform(0.8, 0.95)\n",
    "        else:\n",
    "            # Original performance (undamaged)\n",
    "            concept_perf = np.random.uniform(0.8, 0.95)\n",
    "            unrelated_perf = np.random.uniform(0.8, 0.95)\n",
    "        \n",
    "        return concept_perf, unrelated_perf\n",
    "    \n",
    "    def validate_concept_vector(self, concept_vector: ConceptVector) -> ValidationResult:\n",
    "        \"\"\"\n",
    "        Perform causal validation on a single concept vector\n",
    "        \"\"\"\n",
    "        # Test original performance\n",
    "        orig_concept_perf, orig_unrelated_perf = self.simulate_model_performance(\n",
    "            concept_vector.concept_name, is_damaged=False\n",
    "        )\n",
    "        \n",
    "        # Damage the vector\n",
    "        damaged_vector = self.damage_vector(concept_vector.vector)\n",
    "        \n",
    "        # Test damaged performance\n",
    "        damaged_concept_perf, damaged_unrelated_perf = self.simulate_model_performance(\n",
    "            concept_vector.concept_name, is_damaged=True\n",
    "        )\n",
    "        \n",
    "        # Calculate performance drops\n",
    "        concept_drop = orig_concept_perf - damaged_concept_perf\n",
    "        unrelated_drop = orig_unrelated_perf - damaged_unrelated_perf\n",
    "        \n",
    "        # Validation criteria\n",
    "        is_causally_important = (\n",
    "            concept_drop > 0.2 and  # Significant drop in concept performance\n",
    "            unrelated_drop < 0.1    # Minimal impact on unrelated performance\n",
    "        )\n",
    "        \n",
    "        return ValidationResult(\n",
    "            concept_vector_id=concept_vector.get_id(),\n",
    "            concept_name=concept_vector.concept_name,\n",
    "            original_concept_performance=orig_concept_perf,\n",
    "            damaged_concept_performance=damaged_concept_perf,\n",
    "            original_unrelated_performance=orig_unrelated_perf,\n",
    "            damaged_unrelated_performance=damaged_unrelated_perf,\n",
    "            concept_performance_drop=concept_drop,\n",
    "            unrelated_performance_drop=unrelated_drop,\n",
    "            is_causally_important=is_causally_important\n",
    "        )\n",
    "\n",
    "def causal_validation_stage(finder: ConceptVectorFinder) -> List[ValidationResult]:\n",
    "    \"\"\"\n",
    "    Perform causal validation on all scored concept vectors\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"EXECUTING STAGE 3: CAUSAL VALIDATION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    validator = CausalValidator()\n",
    "    validation_results = []\n",
    "    validated_vectors = []\n",
    "    \n",
    "    print(f\"üß™ Testing {len(finder.concept_vectors)} concept vectors...\")\n",
    "    \n",
    "    for concept_vector in tqdm(finder.concept_vectors, desc=\"Validating vectors\"):\n",
    "        result = validator.validate_concept_vector(concept_vector)\n",
    "        validation_results.append(result)\n",
    "        \n",
    "        if result.is_causally_important:\n",
    "            validated_vectors.append(concept_vector)\n",
    "    \n",
    "    print(f\"‚úì Causal validation completed\")\n",
    "    print(f\"‚úì Validated concept vectors: {len(validated_vectors)}/{len(finder.concept_vectors)}\")\n",
    "    print(f\"‚úì Final success rate: {len(validated_vectors)/len(finder.concept_vectors)*100:.1f}%\")\n",
    "    \n",
    "    # Analysis of validation results\n",
    "    concept_drops = [r.concept_performance_drop for r in validation_results]\n",
    "    unrelated_drops = [r.unrelated_performance_drop for r in validation_results]\n",
    "    \n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Performance drops scatter plot\n",
    "    ax1.scatter(concept_drops, unrelated_drops, alpha=0.6)\n",
    "    ax1.axhline(y=0.1, color='red', linestyle='--', label='Unrelated threshold (0.1)')\n",
    "    ax1.axvline(x=0.2, color='red', linestyle='--', label='Concept threshold (0.2)')\n",
    "    ax1.set_xlabel('Concept Performance Drop')\n",
    "    ax1.set_ylabel('Unrelated Performance Drop')\n",
    "    ax1.set_title('Causal Validation Results')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Validation success by concept\n",
    "    concept_validation = defaultdict(lambda: {'total': 0, 'validated': 0})\n",
    "    for result in validation_results:\n",
    "        concept_validation[result.concept_name]['total'] += 1\n",
    "        if result.is_causally_important:\n",
    "            concept_validation[result.concept_name]['validated'] += 1\n",
    "    \n",
    "    concepts = list(concept_validation.keys())\n",
    "    success_rates = [concept_validation[c]['validated'] / concept_validation[c]['total'] \n",
    "                    for c in concepts]\n",
    "    \n",
    "    ax2.bar(concepts, success_rates)\n",
    "    ax2.set_xlabel('Concept Type')\n",
    "    ax2.set_ylabel('Validation Success Rate')\n",
    "    ax2.set_title('Validation Success by Concept')\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Performance distributions\n",
    "    ax3.hist(concept_drops, bins=20, alpha=0.7, label='Concept drops')\n",
    "    ax3.axvline(x=0.2, color='red', linestyle='--', label='Threshold')\n",
    "    ax3.set_xlabel('Performance Drop')\n",
    "    ax3.set_ylabel('Count')\n",
    "    ax3.set_title('Concept Performance Drop Distribution')\n",
    "    ax3.legend()\n",
    "    \n",
    "    ax4.hist(unrelated_drops, bins=20, alpha=0.7, label='Unrelated drops', color='orange')\n",
    "    ax4.axvline(x=0.1, color='red', linestyle='--', label='Threshold')\n",
    "    ax4.set_xlabel('Performance Drop')\n",
    "    ax4.set_ylabel('Count')\n",
    "    ax4.set_title('Unrelated Performance Drop Distribution')\n",
    "    ax4.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return validation_results\n",
    "\n",
    "# Execute Stage 3\n",
    "validation_results = causal_validation_stage(finder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ab0d14",
   "metadata": {},
   "source": [
    "## Final Analysis and Results\n",
    "\n",
    "This section provides a comprehensive summary of the concept vector finding procedure and results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ebeb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_final_report(finder: ConceptVectorFinder, validation_results: List[ValidationResult]):\n",
    "    \"\"\"\n",
    "    Generate comprehensive report of the concept vector finding procedure\n",
    "    \"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"üéâ CONCEPT VECTOR FINDING - FINAL REPORT (CUDA VERSION)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Summary statistics\n",
    "    total_candidates = len(finder.candidate_vectors)\n",
    "    filtered_candidates = len(finder.filtered_vectors)\n",
    "    concept_candidates = len(finder.concept_vectors)\n",
    "    validated_vectors = sum(1 for r in validation_results if r.is_causally_important)\n",
    "    \n",
    "    print(f\"\\nüìä PIPELINE SUMMARY:\")\n",
    "    print(f\"  Stage 1 - Initial candidates: {total_candidates:,}\")\n",
    "    print(f\"  Stage 1 - After filtering (70%): {filtered_candidates:,}\")\n",
    "    print(f\"  Stage 2 - After scoring: {concept_candidates:,}\")\n",
    "    print(f\"  Stage 3 - Causally validated: {validated_vectors:,}\")\n",
    "    print(f\"  Final success rate: {validated_vectors/total_candidates*100:.2f}%\")\n",
    "    \n",
    "    # Computational complexity achieved\n",
    "    actual_flops = total_candidates * finder.config.vocab_size * finder.config.hidden_dim\n",
    "    print(f\"\\nüíª COMPUTATIONAL COMPLEXITY:\")\n",
    "    print(f\"  Actual FLOPs executed: {actual_flops:,} ({actual_flops/1e9:.2f} GFLOPs)\")\n",
    "    print(f\"  Full-scale estimate: {finder.config.total_candidate_vectors * finder.config.vocab_size * finder.config.hidden_dim:,} ({finder.config.total_candidate_vectors * finder.config.vocab_size * finder.config.hidden_dim/1e12:.2f} TFLOPs)\")\n",
    "    \n",
    "    # GPU utilization\n",
    "    print(f\"\\nüöÄ GPU UTILIZATION:\")\n",
    "    print(f\"  Device: {device} ({torch.cuda.get_device_name()})\")\n",
    "    print(f\"  Peak memory allocated: {torch.cuda.max_memory_allocated() / 1e9:.2f} GB\")\n",
    "    print(f\"  Peak memory reserved: {torch.cuda.max_memory_reserved() / 1e9:.2f} GB\")\n",
    "    \n",
    "    # Concept distribution\n",
    "    concept_dist = defaultdict(int)\n",
    "    validated_concepts = defaultdict(int)\n",
    "    \n",
    "    for cv in finder.concept_vectors:\n",
    "        concept_dist[cv.concept_name] += 1\n",
    "    \n",
    "    for result in validation_results:\n",
    "        if result.is_causally_important:\n",
    "            validated_concepts[result.concept_name] += 1\n",
    "    \n",
    "    print(f\"\\nüéØ CONCEPT DISTRIBUTION:\")\n",
    "    for concept in sorted(concept_dist.keys()):\n",
    "        found = concept_dist[concept]\n",
    "        validated = validated_concepts[concept]\n",
    "        print(f\"  {concept}: {found} found, {validated} validated ({validated/found*100:.1f}% validation rate)\")\n",
    "    \n",
    "    # Performance analysis\n",
    "    concept_drops = [r.concept_performance_drop for r in validation_results if r.is_causally_important]\n",
    "    unrelated_drops = [r.unrelated_performance_drop for r in validation_results if r.is_causally_important]\n",
    "    \n",
    "    if concept_drops:\n",
    "        print(f\"\\nüìà VALIDATION PERFORMANCE:\")\n",
    "        print(f\"  Average concept performance drop: {np.mean(concept_drops):.3f} ¬± {np.std(concept_drops):.3f}\")\n",
    "        print(f\"  Average unrelated performance drop: {np.mean(unrelated_drops):.3f} ¬± {np.std(unrelated_drops):.3f}\")\n",
    "        print(f\"  Selectivity ratio: {np.mean(concept_drops)/np.mean(unrelated_drops):.2f}x\")\n",
    "    \n",
    "    # Best concept vectors\n",
    "    print(f\"\\nüèÜ TOP VALIDATED CONCEPT VECTORS:\")\n",
    "    validated_results = [r for r in validation_results if r.is_causally_important]\n",
    "    top_results = sorted(validated_results, key=lambda x: x.concept_performance_drop, reverse=True)[:5]\n",
    "    \n",
    "    for i, result in enumerate(top_results, 1):\n",
    "        print(f\"  {i}. {result.concept_vector_id} ({result.concept_name})\")\n",
    "        print(f\"     Concept drop: {result.concept_performance_drop:.3f}\")\n",
    "        print(f\"     Unrelated drop: {result.unrelated_performance_drop:.3f}\")\n",
    "    \n",
    "    # Save results\n",
    "    results_summary = {\n",
    "        'pipeline_stats': {\n",
    "            'total_candidates': total_candidates,\n",
    "            'filtered_candidates': filtered_candidates,\n",
    "            'concept_candidates': concept_candidates,\n",
    "            'validated_vectors': validated_vectors,\n",
    "            'success_rate': validated_vectors/total_candidates\n",
    "        },\n",
    "        'computational_complexity': {\n",
    "            'actual_flops': actual_flops,\n",
    "            'fullscale_estimate_flops': finder.config.total_candidate_vectors * finder.config.vocab_size * finder.config.hidden_dim\n",
    "        },\n",
    "        'gpu_info': {\n",
    "            'device': str(device),\n",
    "            'device_name': torch.cuda.get_device_name(),\n",
    "            'peak_memory_allocated_gb': torch.cuda.max_memory_allocated() / 1e9,\n",
    "            'peak_memory_reserved_gb': torch.cuda.max_memory_reserved() / 1e9\n",
    "        },\n",
    "        'concept_distribution': dict(concept_dist),\n",
    "        'validated_concepts': dict(validated_concepts),\n",
    "        'validation_results': [\n",
    "            {\n",
    "                'id': r.concept_vector_id,\n",
    "                'concept': r.concept_name,\n",
    "                'concept_drop': r.concept_performance_drop,\n",
    "                'unrelated_drop': r.unrelated_performance_drop,\n",
    "                'validated': r.is_causally_important\n",
    "            }\n",
    "            for r in validation_results\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Save to file\n",
    "    with open('concept_vector_results_cuda.json', 'w') as f:\n",
    "        json.dump(results_summary, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nüíæ Results saved to: concept_vector_results_cuda.json\")\n",
    "    \n",
    "    return results_summary\n",
    "\n",
    "def visualize_complete_pipeline(finder: ConceptVectorFinder, validation_results: List[ValidationResult]):\n",
    "    \"\"\"\n",
    "    Create comprehensive visualization of the entire pipeline\n",
    "    \"\"\"\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Pipeline funnel\n",
    "    stages = ['Initial\\nCandidates', 'After\\nFiltering', 'After\\nScoring', 'Causally\\nValidated']\n",
    "    counts = [\n",
    "        len(finder.candidate_vectors),\n",
    "        len(finder.filtered_vectors),\n",
    "        len(finder.concept_vectors),\n",
    "        sum(1 for r in validation_results if r.is_causally_important)\n",
    "    ]\n",
    "    \n",
    "    colors = ['lightblue', 'lightgreen', 'orange', 'red']\n",
    "    bars = ax1.bar(stages, counts, color=colors, alpha=0.7)\n",
    "    ax1.set_ylabel('Number of Vectors')\n",
    "    ax1.set_title('Concept Vector Finding Pipeline (CUDA)')\n",
    "    \n",
    "    # Add count labels on bars\n",
    "    for bar, count in zip(bars, counts):\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
    "                f'{count:,}', ha='center', va='bottom')\n",
    "    \n",
    "    # Layer distribution of final vectors\n",
    "    validated_vectors = [r for r in validation_results if r.is_causally_important]\n",
    "    if validated_vectors:\n",
    "        # Extract layer info from concept vector IDs\n",
    "        layers = []\n",
    "        for cv in finder.concept_vectors:\n",
    "            if any(r.concept_vector_id == cv.get_id() and r.is_causally_important for r in validation_results):\n",
    "                layers.append(cv.layer_idx)\n",
    "        \n",
    "        if layers:\n",
    "            ax2.hist(layers, bins=range(finder.config.num_layers + 1), alpha=0.7, color='green')\n",
    "            ax2.set_xlabel('Layer Index')\n",
    "            ax2.set_ylabel('Number of Concept Vectors')\n",
    "            ax2.set_title('Distribution Across Transformer Layers')\n",
    "    \n",
    "    # Concept type distribution\n",
    "    concept_counts = defaultdict(int)\n",
    "    for cv in finder.concept_vectors:\n",
    "        if any(r.concept_vector_id == cv.get_id() and r.is_causally_important for r in validation_results):\n",
    "            concept_counts[cv.concept_name] += 1\n",
    "    \n",
    "    if concept_counts:\n",
    "        concepts, counts = zip(*concept_counts.items())\n",
    "        ax3.pie(counts, labels=concepts, autopct='%1.1f%%', startangle=90)\n",
    "        ax3.set_title('Validated Concept Types')\n",
    "    \n",
    "    # Performance validation scatter\n",
    "    concept_drops = [r.concept_performance_drop for r in validation_results]\n",
    "    unrelated_drops = [r.unrelated_performance_drop for r in validation_results]\n",
    "    validated = [r.is_causally_important for r in validation_results]\n",
    "    \n",
    "    colors = ['red' if v else 'blue' for v in validated]\n",
    "    ax4.scatter(concept_drops, unrelated_drops, c=colors, alpha=0.6)\n",
    "    ax4.axhline(y=0.1, color='red', linestyle='--', alpha=0.5)\n",
    "    ax4.axvline(x=0.2, color='red', linestyle='--', alpha=0.5)\n",
    "    ax4.set_xlabel('Concept Performance Drop')\n",
    "    ax4.set_ylabel('Unrelated Performance Drop')\n",
    "    ax4.set_title('Causal Validation Results')\n",
    "    ax4.legend(['Threshold lines', 'Not validated', 'Validated'])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Generate final report and visualizations\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GENERATING FINAL ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "final_results = generate_final_report(finder, validation_results)\n",
    "visualize_complete_pipeline(finder, validation_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a4abd8",
   "metadata": {},
   "source": [
    "## GPU Memory Management and Cleanup\n",
    "\n",
    "Important functions for managing GPU memory during large-scale experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f784052",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_gpu_memory():\n",
    "    \"\"\"\n",
    "    Properly cleanup GPU memory and model resources (single GPU version)\n",
    "    \"\"\"\n",
    "    global model, tokenizer, finder\n",
    "    \n",
    "    print(\"üßπ Cleaning up GPU memory (single GPU)...\")\n",
    "    \n",
    "    # Clear model references\n",
    "    if 'model' in globals() and model is not None:\n",
    "        # Move model to CPU first\n",
    "        try:\n",
    "            model = model.cpu()\n",
    "            print(\"‚úì Model moved to CPU\")\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Delete model\n",
    "        del model\n",
    "        model = None\n",
    "        print(\"‚úì Model deleted\")\n",
    "    \n",
    "    # Clear tokenizer\n",
    "    if 'tokenizer' in globals() and tokenizer is not None:\n",
    "        del tokenizer\n",
    "        tokenizer = None\n",
    "        print(\"‚úì Tokenizer deleted\")\n",
    "    \n",
    "    # Clear finder and its components\n",
    "    if 'finder' in globals() and finder is not None:\n",
    "        # Clear extracted model components (move to CPU first if they're tensors)\n",
    "        if hasattr(finder, 'embedding_matrix') and finder.embedding_matrix is not None:\n",
    "            if torch.is_tensor(finder.embedding_matrix):\n",
    "                finder.embedding_matrix = finder.embedding_matrix.cpu()\n",
    "            finder.embedding_matrix = None\n",
    "            \n",
    "        if hasattr(finder, 'mlp_weights') and finder.mlp_weights is not None:\n",
    "            for i, weight in enumerate(finder.mlp_weights):\n",
    "                if torch.is_tensor(weight):\n",
    "                    finder.mlp_weights[i] = weight.cpu()\n",
    "            finder.mlp_weights = None\n",
    "            \n",
    "        finder.candidate_vectors = []\n",
    "        finder.filtered_vectors = []\n",
    "        finder.concept_vectors = []\n",
    "        del finder\n",
    "        finder = None\n",
    "        print(\"‚úì ConceptVectorFinder deleted\")\n",
    "    \n",
    "    # Force garbage collection\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    \n",
    "    # Clear CUDA cache\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"‚úì CUDA cache cleared\")\n",
    "    \n",
    "    print(f\"üîç GPU memory after cleanup:\")\n",
    "    print(f\"   Allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "    print(f\"   Reserved: {torch.cuda.memory_reserved() / 1e9:.2f} GB\")\n",
    "    \n",
    "    print(\"\\n‚úÖ GPU memory cleanup completed!\")\n",
    "\n",
    "def check_gpu_memory():\n",
    "    \"\"\"\n",
    "    Check current GPU memory usage (single GPU version)\n",
    "    \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        total_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "        allocated = torch.cuda.memory_allocated() / 1e9\n",
    "        reserved = torch.cuda.memory_reserved() / 1e9\n",
    "        \n",
    "        print(f\"üìä GPU Memory Status (Single GPU):\")\n",
    "        print(f\"   Device: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"   Total: {total_memory:.1f} GB\")\n",
    "        print(f\"   Allocated: {allocated:.2f} GB ({allocated/total_memory*100:.1f}%)\")\n",
    "        print(f\"   Reserved: {reserved:.2f} GB ({reserved/total_memory*100:.1f}%)\")\n",
    "        print(f\"   Available: {total_memory - reserved:.2f} GB\")\n",
    "        print(f\"   Max allocated: {torch.cuda.max_memory_allocated() / 1e9:.2f} GB\")\n",
    "        print(f\"   Max reserved: {torch.cuda.max_memory_reserved() / 1e9:.2f} GB\")\n",
    "        \n",
    "        # Cluster-friendly warnings\n",
    "        if reserved/total_memory > 0.85:\n",
    "            print(\"‚ö†Ô∏è  High memory usage - consider reducing batch size for cluster sharing\")\n",
    "        elif reserved/total_memory < 0.5:\n",
    "            print(\"‚úì Good memory usage for shared cluster environment\")\n",
    "    else:\n",
    "        print(\"‚ùå CUDA not available\")\n",
    "\n",
    "def save_checkpoint(finder, stage_name, additional_data=None):\n",
    "    \"\"\"\n",
    "    Save checkpoint for long-running cluster jobs\n",
    "    \"\"\"\n",
    "    checkpoint_dir = Path(\"checkpoints\")\n",
    "    checkpoint_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    checkpoint_file = checkpoint_dir / f\"concept_vectors_{stage_name}_{timestamp}.pkl\"\n",
    "    \n",
    "    checkpoint_data = {\n",
    "        'config': finder.config,\n",
    "        'candidate_vectors': finder.candidate_vectors,\n",
    "        'filtered_vectors': finder.filtered_vectors,\n",
    "        'concept_vectors': finder.concept_vectors,\n",
    "        'timestamp': timestamp,\n",
    "        'stage': stage_name\n",
    "    }\n",
    "    \n",
    "    if additional_data:\n",
    "        checkpoint_data.update(additional_data)\n",
    "    \n",
    "    with open(checkpoint_file, 'wb') as f:\n",
    "        pickle.dump(checkpoint_data, f)\n",
    "    \n",
    "    print(f\"üíæ Checkpoint saved: {checkpoint_file}\")\n",
    "    return checkpoint_file\n",
    "\n",
    "# Check final memory usage\n",
    "print(\"Final GPU memory status (single GPU):\")\n",
    "check_gpu_memory()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéâ CONCEPT VECTOR FINDING COMPLETED (SINGLE GPU)!\")\n",
    "print(\"=\"*60)\n",
    "print(\"Optimized for shared cluster environment with single Quadro P6000\")\n",
    "print(\"To clean up GPU memory, run: cleanup_gpu_memory()\")\n",
    "print(\"To check memory usage anytime, run: check_gpu_memory()\")\n",
    "print(\"To save checkpoint, run: save_checkpoint(finder, 'stage_name')\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
