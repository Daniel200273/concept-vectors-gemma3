{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bb3ba36",
   "metadata": {},
   "source": [
    "# Concept Vector Finding in Gemma 3 1B\n",
    "\n",
    "This notebook implements the procedure for finding concept vectors in the Gemma 3 1B model as described in the research methodology. The process involves three main stages:\n",
    "\n",
    "1. **Locating Concept Vectors in MLP Layers** - Candidate identification and initial filtering\n",
    "2. **Automated Scoring and Manual Review** - Using external LLM for concept evaluation  \n",
    "3. **Causal Validation** - Vector damage testing to confirm causal importance\n",
    "\n",
    "## Model Architecture Specifications for Gemma 3 1B:\n",
    "- **Number of Layers (L)**: 18 transformer layers\n",
    "- **Hidden Dimension (d)**: 2,048\n",
    "- **Intermediate MLP Dimension (di)**: 8,192 (4x hidden dimension)\n",
    "- **Vocabulary Size (|V|)**: 32,768 tokens\n",
    "- **Total Candidate Vectors**: L √ó di = 18 √ó 8,192 = 147,456"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa381181",
   "metadata": {},
   "source": [
    "## Setup Instructions for Real Gemma Model\n",
    "\n",
    "### Prerequisites for macOS (Apple Silicon)\n",
    "\n",
    "1. **Using Conda (Recommended)**:\n",
    "```bash\n",
    "# Run the automated setup script\n",
    "chmod +x setup_environment.sh\n",
    "./setup_environment.sh\n",
    "\n",
    "# Or manually:\n",
    "conda env create -f environment.yml\n",
    "conda activate gemma_concept_env\n",
    "```\n",
    "\n",
    "2. **System Requirements**:\n",
    "   - **RAM**: 8-12GB for model inference (Apple Silicon optimized)\n",
    "   - **Storage**: ~3GB for model weights\n",
    "   - **Time**: Initial download may take 5-10 minutes\n",
    "\n",
    "3. **Apple Silicon Optimization**:\n",
    "   - PyTorch will automatically use Metal Performance Shaders (MPS)\n",
    "   - No CUDA required - Apple's Metal provides GPU acceleration\n",
    "   - Fallback to CPU if MPS unavailable\n",
    "\n",
    "### Model Variants Available\n",
    "\n",
    "- `google/gemma-2-2b-it`: Instruction-tuned 2B model (recommended for Apple Silicon)\n",
    "- `google/gemma-2-9b-it`: Larger 9B model (requires more memory)\n",
    "- Alternative models for testing if Gemma unavailable\n",
    "\n",
    "### Notes\n",
    "\n",
    "- The notebook automatically detects Apple Silicon and uses MPS backend\n",
    "- Real model extraction provides authentic vocabulary projections and concept vectors\n",
    "- Metal Performance Shaders provide excellent acceleration on M1/M2/M3 chips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc1b843c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemma Model Configuration:\n",
      "  Layers: 26\n",
      "  Hidden Dimension: 1152\n",
      "  MLP Dimension: 6912\n",
      "  Vocabulary Size: 262145\n",
      "  Total Candidate Vectors: 179,712\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "import subprocess\n",
    "import requests\n",
    "import time\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "from dataclasses import dataclass\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Hugging Face imports for Gemma model\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Configuration for Gemma model architecture\n",
    "@dataclass\n",
    "class GemmaConfig:\n",
    "    num_layers: int = 18\n",
    "    hidden_dim: int = 2048\n",
    "    mlp_dim: int = 8192  # 4x hidden_dim\n",
    "    vocab_size: int = 32768\n",
    "    total_candidate_vectors: int = 18 * 8192  # 147,456\n",
    "    \n",
    "    @classmethod\n",
    "    def from_model(cls, model, tokenizer):\n",
    "        \"\"\"Create configuration from actual model\"\"\"\n",
    "        if model is None:\n",
    "            return cls()  # Use defaults if model not loaded\n",
    "            \n",
    "        config_dict = model.config.to_dict()\n",
    "        \n",
    "        # Extract relevant parameters\n",
    "        num_layers = config_dict.get('num_hidden_layers', 18)\n",
    "        hidden_dim = config_dict.get('hidden_size', 2048)\n",
    "        intermediate_size = config_dict.get('intermediate_size', hidden_dim * 4)\n",
    "        vocab_size = len(tokenizer) if tokenizer else 32768\n",
    "        \n",
    "        return cls(\n",
    "            num_layers=num_layers,\n",
    "            hidden_dim=hidden_dim,\n",
    "            mlp_dim=intermediate_size,\n",
    "            vocab_size=vocab_size,\n",
    "            total_candidate_vectors=num_layers * intermediate_size\n",
    "        )\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        print(f\"Gemma Model Configuration:\")\n",
    "        print(f\"  Layers: {self.num_layers}\")\n",
    "        print(f\"  Hidden Dimension: {self.hidden_dim}\")\n",
    "        print(f\"  MLP Dimension: {self.mlp_dim}\")\n",
    "        print(f\"  Vocabulary Size: {self.vocab_size}\")\n",
    "        print(f\"  Total Candidate Vectors: {self.total_candidate_vectors:,}\")\n",
    "\n",
    "# Create configuration from loaded model or use defaults\n",
    "config = GemmaConfig.from_model(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a0c1845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Gemma model from Hugging Face...\n",
      "Note: This requires ~3GB of memory for model weights\n",
      "Using device: mps\n",
      "üçé Apple Silicon detected - using Metal Performance Shaders for acceleration\n",
      "üì• Downloading tokenizer...\n",
      "üì• Downloading model (this may take a few minutes)...\n",
      "üì• Downloading model (this may take a few minutes)...\n",
      "‚úì Model loaded successfully: google/gemma-3-1b-it\n",
      "‚úì Model device: mps:0\n",
      "‚úì Model dtype: torch.float16\n",
      "‚úì Vocabulary size: 262145\n",
      "‚úì Model configuration loaded\n",
      "‚úì Model loaded successfully: google/gemma-3-1b-it\n",
      "‚úì Model device: mps:0\n",
      "‚úì Model dtype: torch.float16\n",
      "‚úì Vocabulary size: 262145\n",
      "‚úì Model configuration loaded\n"
     ]
    }
   ],
   "source": [
    "# Install required packages (run once)\n",
    "# !pip install transformers torch accelerate\n",
    "\n",
    "# Load Gemma model from Hugging Face\n",
    "print(\"Loading Gemma model from Hugging Face...\")\n",
    "print(\"Note: This requires ~3GB of memory for model weights\")\n",
    "\n",
    "# Model configuration\n",
    "model_name = \"google/gemma-3-1b-it\" \n",
    "\n",
    "\n",
    "# Set device - optimized for Apple Silicon\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "if device == \"mps\":\n",
    "    print(\"üçé Apple Silicon detected - using Metal Performance Shaders for acceleration\")\n",
    "\n",
    "# Load tokenizer and model\n",
    "try:\n",
    "    print(\"üì• Downloading tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    print(\"üì• Downloading model (this may take a few minutes)...\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16 if device in [\"cuda\", \"mps\"] else torch.float32,\n",
    "        device_map=\"auto\" if device == \"cuda\" else None,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    # Move model to appropriate device\n",
    "    if device in [\"cpu\", \"mps\"]:\n",
    "        model = model.to(device)\n",
    "    \n",
    "    print(f\"‚úì Model loaded successfully: {model_name}\")\n",
    "    print(f\"‚úì Model device: {next(model.parameters()).device}\")\n",
    "    print(f\"‚úì Model dtype: {next(model.parameters()).dtype}\")\n",
    "    print(f\"‚úì Vocabulary size: {len(tokenizer)}\")\n",
    "    \n",
    "    # Get actual model configuration\n",
    "    config_dict = model.config.to_dict()\n",
    "    print(f\"‚úì Model configuration loaded\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading model: {e}\")\n",
    "    print(\"Please ensure you have sufficient memory and internet connection\")\n",
    "    print(\"Falling back to simulated mode...\")\n",
    "    model = None\n",
    "    tokenizer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ea24281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "COMPUTATIONAL COMPLEXITY ANALYSIS\n",
      "============================================================\n",
      "Stage 1 - Vocabulary Projections:\n",
      "  FLOPs per projection: 301,991,040\n",
      "  Total candidate vectors: 179,712\n",
      "  Total projection FLOPs: 54,271,413,780,480 (54.27 TFLOPs)\n",
      "\n",
      "Stage 2 - External LLM Scoring:\n",
      "  Vectors after 30% filtering: 125,798\n",
      "  API calls needed: 125,798\n",
      "  Estimated cost (assuming $0.01/call): $1,257.98\n",
      "\n",
      "Stage 3 - Causal Validation:\n",
      "  Estimated concept vectors found: 12,579\n",
      "  Vector damage operations: 12579 (trivial complexity)\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class ConceptVector:\n",
    "    \"\"\"Represents a concept vector candidate with its metadata\"\"\"\n",
    "    layer_idx: int\n",
    "    vector_idx: int\n",
    "    vector: np.ndarray\n",
    "    vocab_projection: np.ndarray\n",
    "    top_tokens: List[Tuple[str, float]]\n",
    "    concept_score: float = 0.0\n",
    "    concept_name: str = \"\"\n",
    "    is_validated: bool = False\n",
    "    \n",
    "    def get_id(self) -> str:\n",
    "        return f\"L{self.layer_idx}_V{self.vector_idx}\"\n",
    "\n",
    "class ConceptVectorFinder:\n",
    "    \"\"\"Main class for finding concept vectors in Gemma model\"\"\"\n",
    "    \n",
    "    def __init__(self, config: GemmaConfig, model=None, tokenizer=None):\n",
    "        self.config = config\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.candidate_vectors: List[ConceptVector] = []\n",
    "        self.filtered_vectors: List[ConceptVector] = []\n",
    "        self.concept_vectors: List[ConceptVector] = []\n",
    "        \n",
    "        # Model components (will be extracted from actual model)\n",
    "        self.embedding_matrix = None  # E: (vocab_size, hidden_dim)\n",
    "        self.mlp_weights = None       # List of MLP weight matrices for each layer\n",
    "        \n",
    "    def initialize_model_components(self):\n",
    "        \"\"\"Extract model components from the actual Gemma model\"\"\"\n",
    "        if self.model is None:\n",
    "            print(\"‚ö†Ô∏è  No model loaded, using simulated components...\")\n",
    "            self._initialize_simulated_components()\n",
    "            return\n",
    "            \n",
    "        print(\"Extracting components from real Gemma model...\")\n",
    "        \n",
    "        try:\n",
    "            # Extract embedding matrix\n",
    "            # In Gemma, embeddings are in model.model.embed_tokens.weight\n",
    "            self.embedding_matrix = self.model.model.embed_tokens.weight.data.cpu().numpy()\n",
    "            print(f\"‚úì Embedding matrix extracted: {self.embedding_matrix.shape}\")\n",
    "            \n",
    "            # Extract MLP weights from each transformer layer\n",
    "            self.mlp_weights = []\n",
    "            self.vocab_tokens = []\n",
    "            \n",
    "            # Get vocabulary tokens\n",
    "            if self.tokenizer:\n",
    "                self.vocab_tokens = [self.tokenizer.decode([i]) for i in range(len(self.tokenizer))]\n",
    "            else:\n",
    "                self.vocab_tokens = [f\"token_{i}\" for i in range(self.config.vocab_size)]\n",
    "            \n",
    "            # Extract MLP weights from each layer\n",
    "            for layer_idx in range(self.config.num_layers):\n",
    "                # Access the MLP layer in the transformer\n",
    "                # Gemma structure: model.model.layers[i].mlp.up_proj, gate_proj, down_proj\n",
    "                mlp_layer = self.model.model.layers[layer_idx].mlp\n",
    "                \n",
    "                # We want the up_proj weights which map hidden_dim -> intermediate_dim\n",
    "                # up_proj.weight shape: (intermediate_dim, hidden_dim)\n",
    "                # We transpose to get (hidden_dim, intermediate_dim) for our projection\n",
    "                up_proj_weight = mlp_layer.up_proj.weight.data.cpu().numpy().T\n",
    "                self.mlp_weights.append(up_proj_weight)\n",
    "                \n",
    "            print(f\"‚úì MLP weights extracted from {len(self.mlp_weights)} layers\")\n",
    "            print(f\"‚úì Each MLP weight shape: {self.mlp_weights[0].shape}\")\n",
    "            print(f\"‚úì Vocabulary tokens: {len(self.vocab_tokens)}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error extracting model components: {e}\")\n",
    "            print(\"Falling back to simulated components...\")\n",
    "            self._initialize_simulated_components()\n",
    "    \n",
    "    def _initialize_simulated_components(self):\n",
    "        \"\"\"Initialize simulated model components for demonstration\"\"\"\n",
    "        print(\"Initializing simulated model components...\")\n",
    "        \n",
    "        # Simulate embedding matrix E (vocab_size √ó hidden_dim)\n",
    "        self.embedding_matrix = np.random.randn(self.config.vocab_size, self.config.hidden_dim).astype(np.float32)\n",
    "        \n",
    "        # Simulate MLP weight matrices for each layer\n",
    "        self.mlp_weights = []\n",
    "        for layer in range(self.config.num_layers):\n",
    "            # MLP weights W_V (hidden_dim √ó mlp_dim)\n",
    "            mlp_weight = np.random.randn(self.config.hidden_dim, self.config.mlp_dim).astype(np.float32)\n",
    "            self.mlp_weights.append(mlp_weight)\n",
    "            \n",
    "        print(f\"‚úì Simulated embedding matrix shape: {self.embedding_matrix.shape}\")\n",
    "        print(f\"‚úì Simulated MLP weights per layer: {len(self.mlp_weights)} layers\")\n",
    "        print(f\"‚úì Each simulated MLP weight shape: {self.mlp_weights[0].shape}\")\n",
    "        \n",
    "        # Simulate vocabulary tokens\n",
    "        if self.tokenizer:\n",
    "            self.vocab_tokens = [self.tokenizer.decode([i]) for i in range(len(self.tokenizer))]\n",
    "        else:\n",
    "            self.vocab_tokens = [f\"token_{i}\" for i in range(self.config.vocab_size)]\n",
    "        \n",
    "    def estimate_computational_complexity(self):\n",
    "        \"\"\"Estimate the computational complexity of the full procedure\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"COMPUTATIONAL COMPLEXITY ANALYSIS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Stage 1: Vocabulary projections\n",
    "        flops_per_projection = self.config.vocab_size * self.config.hidden_dim\n",
    "        total_projections = self.config.total_candidate_vectors\n",
    "        total_projection_flops = flops_per_projection * total_projections\n",
    "        \n",
    "        print(f\"Stage 1 - Vocabulary Projections:\")\n",
    "        print(f\"  FLOPs per projection: {flops_per_projection:,}\")\n",
    "        print(f\"  Total candidate vectors: {total_projections:,}\")\n",
    "        print(f\"  Total projection FLOPs: {total_projection_flops:,} ({total_projection_flops/1e12:.2f} TFLOPs)\")\n",
    "        \n",
    "        # Stage 2: External LLM scoring (70% of candidates after filtering)\n",
    "        remaining_after_filter = int(total_projections * 0.7)\n",
    "        print(f\"\\nStage 2 - External LLM Scoring:\")\n",
    "        print(f\"  Vectors after 30% filtering: {remaining_after_filter:,}\")\n",
    "        print(f\"  API calls needed: {remaining_after_filter:,}\")\n",
    "        print(f\"  Estimated cost (assuming $0.01/call): ${remaining_after_filter * 0.01:,.2f}\")\n",
    "        \n",
    "        # Stage 3: Causal validation\n",
    "        concept_vectors_found = int(remaining_after_filter * 0.1)  # Estimate 10% pass scoring\n",
    "        print(f\"\\nStage 3 - Causal Validation:\")\n",
    "        print(f\"  Estimated concept vectors found: {concept_vectors_found:,}\")\n",
    "        print(f\"  Vector damage operations: {concept_vectors_found} (trivial complexity)\")\n",
    "        \n",
    "        return {\n",
    "            'projection_flops': total_projection_flops,\n",
    "            'api_calls': remaining_after_filter,\n",
    "            'concept_vectors_estimated': concept_vectors_found\n",
    "        }\n",
    "\n",
    "finder = ConceptVectorFinder(config, model, tokenizer)\n",
    "complexity_stats = finder.estimate_computational_complexity()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1dddb7",
   "metadata": {},
   "source": [
    "## Stage 1: Candidate Identification and Initial Filtering\n",
    "\n",
    "This stage implements the most computationally intensive part of the procedure:\n",
    "1. Extract candidate vectors from MLP layers (147,456 total)\n",
    "2. Project each vector onto vocabulary space using embedding matrix E\n",
    "3. Calculate average logit scores for initial filtering\n",
    "4. Exclude ~30% of candidates with lowest scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ce96c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_candidate_vectors(finder: ConceptVectorFinder, max_vectors_per_layer: int = 100):\n",
    "    \"\"\"\n",
    "    Extract candidate vectors from MLP layers and perform vocabulary projections.\n",
    "    For demonstration, we'll sample a subset of the full 147,456 candidates.\n",
    "    \"\"\"\n",
    "    print(\"Stage 1: Extracting candidate vectors and computing vocabulary projections...\")\n",
    "    \n",
    "    # Initialize model components\n",
    "    finder.initialize_model_components()\n",
    "    \n",
    "    candidates = []\n",
    "    total_processed = 0\n",
    "    \n",
    "    for layer_idx in range(finder.config.num_layers):\n",
    "        print(f\"\\nProcessing layer {layer_idx + 1}/{finder.config.num_layers}\")\n",
    "        \n",
    "        # Get MLP weight matrix for this layer (hidden_dim √ó mlp_dim)\n",
    "        mlp_weight = finder.mlp_weights[layer_idx]\n",
    "        \n",
    "        # Sample candidate vectors (columns of the weight matrix)\n",
    "        # In practice, you'd process all 8,192 vectors per layer\n",
    "        vector_indices = np.random.choice(\n",
    "            finder.config.mlp_dim, \n",
    "            size=min(max_vectors_per_layer, finder.config.mlp_dim), \n",
    "            replace=False\n",
    "        )\n",
    "        \n",
    "        for i, vector_idx in enumerate(tqdm(vector_indices, desc=f\"Layer {layer_idx}\")):\n",
    "            # Extract candidate vector v_‚Ñìj (column j of W_‚ÑìV)\n",
    "            candidate_vector = mlp_weight[:, vector_idx]  # Shape: (hidden_dim,)\n",
    "            \n",
    "            # Project onto vocabulary space: E @ v_‚Ñìj\n",
    "            # E shape: (vocab_size, hidden_dim), v shape: (hidden_dim,)\n",
    "            vocab_projection = finder.embedding_matrix @ candidate_vector  # Shape: (vocab_size,)\n",
    "            \n",
    "            # Get top tokens for this projection\n",
    "            top_k = 50\n",
    "            top_indices = np.argsort(vocab_projection)[-top_k:][::-1]\n",
    "            top_tokens = [(finder.vocab_tokens[idx], float(vocab_projection[idx])) \n",
    "                         for idx in top_indices]\n",
    "            \n",
    "            # Create ConceptVector object\n",
    "            concept_vec = ConceptVector(\n",
    "                layer_idx=layer_idx,\n",
    "                vector_idx=vector_idx,\n",
    "                vector=candidate_vector,\n",
    "                vocab_projection=vocab_projection,\n",
    "                top_tokens=top_tokens\n",
    "            )\n",
    "            \n",
    "            candidates.append(concept_vec)\n",
    "            total_processed += 1\n",
    "    \n",
    "    finder.candidate_vectors = candidates\n",
    "    print(f\"\\n‚úì Extracted {len(candidates):,} candidate vectors\")\n",
    "    print(f\"‚úì Total FLOPs (estimated): {len(candidates) * finder.config.vocab_size * finder.config.hidden_dim:,}\")\n",
    "    \n",
    "    return candidates\n",
    "\n",
    "def filter_candidates_by_score(finder: ConceptVectorFinder, exclusion_ratio: float = 0.3):\n",
    "    \"\"\"\n",
    "    Filter candidates based on average logit scores, excluding bottom 30%\n",
    "    \"\"\"\n",
    "    print(f\"\\nFiltering candidates (excluding bottom {exclusion_ratio*100:.0f}%)...\")\n",
    "    \n",
    "    # Calculate average logit score for each candidate\n",
    "    for candidate in finder.candidate_vectors:\n",
    "        candidate.concept_score = np.mean(candidate.vocab_projection)\n",
    "    \n",
    "    # Sort by score and exclude bottom 30%\n",
    "    sorted_candidates = sorted(finder.candidate_vectors, \n",
    "                             key=lambda x: x.concept_score, reverse=True)\n",
    "    \n",
    "    cutoff_idx = int(len(sorted_candidates) * (1 - exclusion_ratio))\n",
    "    finder.filtered_vectors = sorted_candidates[:cutoff_idx]\n",
    "    \n",
    "    print(f\"‚úì Retained {len(finder.filtered_vectors):,} candidates after filtering\")\n",
    "    print(f\"‚úì Excluded {len(finder.candidate_vectors) - len(finder.filtered_vectors):,} candidates\")\n",
    "    \n",
    "    # Show score distribution\n",
    "    scores = [c.concept_score for c in finder.candidate_vectors]\n",
    "    cutoff_score = finder.filtered_vectors[-1].concept_score if finder.filtered_vectors else 0\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(scores, bins=50, alpha=0.7, edgecolor='black')\n",
    "    plt.axvline(cutoff_score, color='red', linestyle='--', \n",
    "                label=f'Cutoff (score={cutoff_score:.3f})')\n",
    "    plt.xlabel('Average Logit Score')\n",
    "    plt.ylabel('Number of Candidates')\n",
    "    plt.title('Distribution of Candidate Vector Scores')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    return finder.filtered_vectors\n",
    "\n",
    "# Execute Stage 1\n",
    "print(\"=\"*60)\n",
    "print(\"EXECUTING STAGE 1: CANDIDATE IDENTIFICATION & FILTERING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Extract candidates (using subset for demonstration)\n",
    "candidates = extract_candidate_vectors(finder, max_vectors_per_layer=50)\n",
    "\n",
    "# Filter candidates\n",
    "filtered_candidates = filter_candidates_by_score(finder, exclusion_ratio=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffb4fc3",
   "metadata": {},
   "source": [
    "## Stage 2: Automated Scoring and Manual Review\n",
    "\n",
    "This stage uses an external LLM (GPT-4 simulation) to score concept clarity:\n",
    "1. Extract top-K tokens from each filtered candidate's vocabulary projection\n",
    "2. Send tokens to external LLM for concept identification and scoring\n",
    "3. Score candidates on clarity and prominence of concepts (0-1 scale)\n",
    "4. Retain only candidates with scores > 0.85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e24c811",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExternalLLMScorer:\n",
    "    \"\"\"Simulates external LLM (GPT-4) scoring for concept identification\"\"\"\n",
    "    \n",
    "    def __init__(self, tokenizer=None):\n",
    "        self.tokenizer = tokenizer\n",
    "        # Enhanced concept patterns for real token analysis\n",
    "        self.concept_patterns = {\n",
    "            'animals': ['cat', 'dog', 'bird', 'fish', 'animal', 'pet', 'wild', 'zoo', 'farm', 'mammal'],\n",
    "            'colors': ['red', 'blue', 'green', 'color', 'bright', 'dark', 'yellow', 'purple', 'orange'],\n",
    "            'numbers': ['one', 'two', 'three', 'number', 'count', 'digit', 'math', 'numeric', 'zero'],\n",
    "            'emotions': ['happy', 'sad', 'angry', 'emotion', 'feel', 'mood', 'joy', 'fear', 'love'],\n",
    "            'technology': ['computer', 'software', 'digital', 'tech', 'device', 'internet', 'code'],\n",
    "            'food': ['eat', 'food', 'meal', 'hungry', 'cooking', 'restaurant', 'kitchen', 'recipe'],\n",
    "            'travel': ['travel', 'trip', 'journey', 'destination', 'flight', 'hotel', 'vacation'],\n",
    "            'science': ['research', 'study', 'experiment', 'data', 'analysis', 'theory', 'hypothesis'],\n",
    "            'language': ['word', 'sentence', 'grammar', 'language', 'speak', 'write', 'text'],\n",
    "            'time': ['time', 'hour', 'day', 'week', 'month', 'year', 'clock', 'calendar'],\n",
    "            'space': ['space', 'planet', 'star', 'universe', 'galaxy', 'earth', 'moon', 'sun'],\n",
    "            'body': ['head', 'hand', 'body', 'heart', 'brain', 'eye', 'arm', 'leg', 'face']\n",
    "        }\n",
    "    \n",
    "    def clean_token(self, token: str) -> str:\n",
    "        \"\"\"Clean tokenizer artifacts from token strings\"\"\"\n",
    "        if not token:\n",
    "            return \"\"\n",
    "        # Remove common tokenizer prefixes/suffixes\n",
    "        token = token.replace('‚ñÅ', ' ')  # SentencePiece underscore\n",
    "        token = token.replace('ƒ†', ' ')   # GPT-style space marker\n",
    "        token = token.strip()\n",
    "        return token.lower()\n",
    "    \n",
    "    def score_concept_vector(self, top_tokens: List[Tuple[str, float]], k: int = 200) -> Tuple[float, str]:\n",
    "        \"\"\"\n",
    "        Simulate GPT-4 scoring of concept vectors based on top tokens\n",
    "        Returns: (score, concept_name)\n",
    "        \"\"\"\n",
    "        # Extract and clean token strings\n",
    "        cleaned_tokens = []\n",
    "        for token, score in top_tokens[:k]:\n",
    "            cleaned = self.clean_token(token)\n",
    "            if cleaned and len(cleaned) > 1:  # Filter out empty and single chars\n",
    "                cleaned_tokens.append(cleaned)\n",
    "        \n",
    "        if not cleaned_tokens:\n",
    "            return 0.0, \"unknown\"\n",
    "        \n",
    "        # Find best matching concept\n",
    "        best_score = 0.0\n",
    "        best_concept = \"unknown\"\n",
    "        \n",
    "        for concept_name, pattern_words in self.concept_patterns.items():\n",
    "            # Calculate overlap score with fuzzy matching\n",
    "            overlap_count = 0\n",
    "            for token in cleaned_tokens:\n",
    "                for pattern_word in pattern_words:\n",
    "                    if pattern_word in token or token in pattern_word:\n",
    "                        overlap_count += 1\n",
    "                        break\n",
    "            \n",
    "            # Calculate relative overlap score\n",
    "            overlap_score = overlap_count / len(pattern_words) if pattern_words else 0\n",
    "            \n",
    "            # Boost score based on token frequency in top positions\n",
    "            position_boost = sum(1/(i+1) for i, token in enumerate(cleaned_tokens[:20]) \n",
    "                                if any(pw in token or token in pw for pw in pattern_words))\n",
    "            \n",
    "            final_score = overlap_score + position_boost * 0.1\n",
    "            \n",
    "            # Add some controlled randomness\n",
    "            final_score += np.random.normal(0, 0.05)\n",
    "            final_score = max(0, min(1, final_score))\n",
    "            \n",
    "            if final_score > best_score:\n",
    "                best_score = final_score\n",
    "                best_concept = concept_name\n",
    "        \n",
    "        # Additional scoring factors for token coherence\n",
    "        token_coherence = self._assess_token_coherence(cleaned_tokens)\n",
    "        combined_score = (best_score * 0.7 + token_coherence * 0.3)\n",
    "        \n",
    "        return combined_score, best_concept\n",
    "    \n",
    "    def _assess_token_coherence(self, tokens: List[str]) -> float:\n",
    "        \"\"\"Assess how coherent the top tokens are as a group\"\"\"\n",
    "        if len(tokens) < 5:\n",
    "            return 0.5\n",
    "            \n",
    "        # Simple heuristics for coherence\n",
    "        coherence_score = 0.0\n",
    "        \n",
    "        # Check for repeated prefixes/suffixes\n",
    "        prefixes = [token[:3] for token in tokens if len(token) >= 3]\n",
    "        prefix_variety = len(set(prefixes)) / len(prefixes) if prefixes else 0\n",
    "        \n",
    "        # Check average token length (very short or very long tokens may be less meaningful)\n",
    "        avg_length = np.mean([len(token) for token in tokens])\n",
    "        length_score = 1.0 - abs(avg_length - 5) / 10  # Optimal around 5 characters\n",
    "        length_score = max(0, min(1, length_score))\n",
    "        \n",
    "        # Combine factors\n",
    "        coherence_score = (prefix_variety * 0.3 + length_score * 0.7)\n",
    "        \n",
    "        return coherence_score\n",
    "\n",
    "def automated_scoring_stage(finder: ConceptVectorFinder, score_threshold: float = 0.85):\n",
    "    \"\"\"\n",
    "    Score filtered candidates using external LLM simulation\n",
    "    \"\"\"\n",
    "    print(\"Stage 2: Automated scoring with external LLM...\")\n",
    "    \n",
    "    scorer = ExternalLLMScorer(finder.tokenizer)\n",
    "    scored_vectors = []\n",
    "    \n",
    "    print(f\"Scoring {len(finder.filtered_vectors)} candidates...\")\n",
    "    \n",
    "    for i, candidate in enumerate(tqdm(finder.filtered_vectors, desc=\"Scoring\")):\n",
    "        # Simulate API call delay (reduced for demo)\n",
    "        time.sleep(0.005)  # Reduced delay\n",
    "        \n",
    "        # Score the candidate\n",
    "        score, concept_name = scorer.score_concept_vector(candidate.top_tokens)\n",
    "        \n",
    "        # Update candidate with scoring results\n",
    "        candidate.concept_score = score\n",
    "        candidate.concept_name = concept_name\n",
    "        \n",
    "        # Keep candidates above threshold\n",
    "        if score >= score_threshold:\n",
    "            scored_vectors.append(candidate)\n",
    "    \n",
    "    finder.concept_vectors = scored_vectors\n",
    "    \n",
    "    print(f\"‚úì Found {len(scored_vectors)} concept vectors above threshold {score_threshold}\")\n",
    "    print(f\"‚úì Success rate: {len(scored_vectors)/len(finder.filtered_vectors)*100:.1f}%\")\n",
    "    \n",
    "    # Show some example top tokens from real model\n",
    "    if scored_vectors and finder.tokenizer:\n",
    "        print(f\"\\nüîç Example top tokens from highest scoring concept vector:\")\n",
    "        best_cv = max(scored_vectors, key=lambda x: x.concept_score)\n",
    "        print(f\"   Concept: {best_cv.concept_name} (score: {best_cv.concept_score:.3f})\")\n",
    "        print(f\"   Top 10 tokens: {[token for token, _ in best_cv.top_tokens[:10]]}\")\n",
    "    \n",
    "    # Show concept distribution\n",
    "    concept_counts = defaultdict(int)\n",
    "    scores = []\n",
    "    \n",
    "    for cv in finder.concept_vectors:\n",
    "        concept_counts[cv.concept_name] += 1\n",
    "        scores.append(cv.concept_score)\n",
    "    \n",
    "    # Plot results\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Score distribution\n",
    "    ax1.hist([c.concept_score for c in finder.filtered_vectors], \n",
    "             bins=30, alpha=0.7, label='All candidates', color='lightblue')\n",
    "    ax1.hist(scores, bins=30, alpha=0.8, label='Concept vectors', color='orange')\n",
    "    ax1.axvline(score_threshold, color='red', linestyle='--', \n",
    "                label=f'Threshold ({score_threshold})')\n",
    "    ax1.set_xlabel('Concept Score')\n",
    "    ax1.set_ylabel('Count')\n",
    "    ax1.set_title('Score Distribution')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Concept distribution\n",
    "    if concept_counts:\n",
    "        concepts, counts = zip(*concept_counts.items())\n",
    "        ax2.bar(concepts, counts)\n",
    "        ax2.set_xlabel('Concept Type')\n",
    "        ax2.set_ylabel('Number of Vectors')\n",
    "        ax2.set_title('Identified Concepts')\n",
    "        ax2.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return scored_vectors\n",
    "\n",
    "def manual_review_simulation(finder: ConceptVectorFinder):\n",
    "    \"\"\"\n",
    "    Simulate manual review process for final verification\n",
    "    \"\"\"\n",
    "    print(\"\\nStage 2b: Manual review simulation...\")\n",
    "    \n",
    "    verified_vectors = []\n",
    "    \n",
    "    for candidate in finder.concept_vectors:\n",
    "        # Simulate human reviewer decisions based on various criteria\n",
    "        top_token_strings = [token for token, _ in candidate.top_tokens[:10]]\n",
    "        \n",
    "        # Simulate review criteria\n",
    "        coherence_score = np.random.uniform(0.6, 1.0)\n",
    "        specificity_score = np.random.uniform(0.5, 1.0)\n",
    "        clarity_score = np.random.uniform(0.7, 1.0)\n",
    "        \n",
    "        # Overall manual score\n",
    "        manual_score = (coherence_score + specificity_score + clarity_score) / 3\n",
    "        \n",
    "        # Accept if both automated and manual scores are high\n",
    "        if manual_score > 0.75 and candidate.concept_score > 0.85:\n",
    "            candidate.is_validated = True\n",
    "            verified_vectors.append(candidate)\n",
    "    \n",
    "    print(f\"‚úì Manual review completed: {len(verified_vectors)}/{len(finder.concept_vectors)} verified\")\n",
    "    \n",
    "    return verified_vectors\n",
    "\n",
    "# Execute Stage 2\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXECUTING STAGE 2: AUTOMATED SCORING & MANUAL REVIEW\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "concept_vectors = automated_scoring_stage(finder, score_threshold=0.75)  # Lowered for demo\n",
    "verified_vectors = manual_review_simulation(finder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a400293",
   "metadata": {},
   "source": [
    "## Stage 3: Causal Validation\n",
    "\n",
    "This stage validates that identified concept vectors genuinely influence model behavior:\n",
    "1. **Vector Damage**: Add Gaussian noise to candidate concept vectors\n",
    "2. **Performance Evaluation**: Test model on concept-related vs unrelated questions  \n",
    "3. **Validation Criterion**: Retain only vectors where damage significantly affects concept-related performance\n",
    "4. **Specificity Check**: Ensure minimal impact on unrelated questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994f4e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ValidationResult:\n",
    "    \"\"\"Results from causal validation testing\"\"\"\n",
    "    concept_vector_id: str\n",
    "    concept_name: str\n",
    "    original_concept_performance: float\n",
    "    damaged_concept_performance: float\n",
    "    original_unrelated_performance: float\n",
    "    damaged_unrelated_performance: float\n",
    "    concept_performance_drop: float\n",
    "    unrelated_performance_drop: float\n",
    "    is_causally_important: bool\n",
    "\n",
    "class CausalValidator:\n",
    "    \"\"\"Implements causal validation through vector damage testing\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Simulate test questions for different concepts\n",
    "        self.test_questions = {\n",
    "            'animals': [\n",
    "                \"What sound does a cat make?\",\n",
    "                \"Name three types of dogs.\",\n",
    "                \"Where do birds build their nests?\"\n",
    "            ],\n",
    "            'colors': [\n",
    "                \"What color is the sky?\",\n",
    "                \"Name the primary colors.\",\n",
    "                \"What happens when you mix red and blue?\"\n",
    "            ],\n",
    "            'numbers': [\n",
    "                \"What comes after the number 5?\",\n",
    "                \"How many sides does a triangle have?\",\n",
    "                \"What is 2 plus 2?\"\n",
    "            ],\n",
    "            'emotions': [\n",
    "                \"How do you feel when you're happy?\",\n",
    "                \"What makes people sad?\",\n",
    "                \"Describe anger.\"\n",
    "            ],\n",
    "            'technology': [\n",
    "                \"What is a computer used for?\",\n",
    "                \"How does the internet work?\",\n",
    "                \"What is artificial intelligence?\"\n",
    "            ],\n",
    "            'unrelated': [\n",
    "                \"What is the weather like?\",\n",
    "                \"How do you cook pasta?\",\n",
    "                \"What is the capital of France?\"\n",
    "            ]\n",
    "        }\n",
    "    \n",
    "    def damage_vector(self, vector: np.ndarray, noise_std: float = 0.1) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Apply Gaussian noise to damage a concept vector\n",
    "        v_‚Ñìj ‚Üê v_‚Ñìj + Œµ, where Œµ ‚àº N(0, noise_std)\n",
    "        \"\"\"\n",
    "        noise = np.random.normal(0, noise_std, vector.shape)\n",
    "        return vector + noise\n",
    "    \n",
    "    def simulate_model_performance(self, concept_name: str, is_damaged: bool = False) -> Tuple[float, float]:\n",
    "        \"\"\"\n",
    "        Simulate model performance on concept-related and unrelated questions\n",
    "        Returns: (concept_performance, unrelated_performance)\n",
    "        \"\"\"\n",
    "        if is_damaged:\n",
    "            # Simulate performance drop for concept-related questions when vector is damaged\n",
    "            if concept_name in self.test_questions:\n",
    "                concept_perf = np.random.uniform(0.3, 0.7)  # Significant drop\n",
    "            else:\n",
    "                concept_perf = np.random.uniform(0.7, 0.9)  # Less affected\n",
    "            \n",
    "            # Unrelated performance should be minimally affected\n",
    "            unrelated_perf = np.random.uniform(0.8, 0.95)\n",
    "        else:\n",
    "            # Original performance (undamaged)\n",
    "            concept_perf = np.random.uniform(0.8, 0.95)\n",
    "            unrelated_perf = np.random.uniform(0.8, 0.95)\n",
    "        \n",
    "        return concept_perf, unrelated_perf\n",
    "    \n",
    "    def validate_concept_vector(self, concept_vector: ConceptVector) -> ValidationResult:\n",
    "        \"\"\"\n",
    "        Perform causal validation on a single concept vector\n",
    "        \"\"\"\n",
    "        # Test original performance\n",
    "        orig_concept_perf, orig_unrelated_perf = self.simulate_model_performance(\n",
    "            concept_vector.concept_name, is_damaged=False\n",
    "        )\n",
    "        \n",
    "        # Damage the vector\n",
    "        damaged_vector = self.damage_vector(concept_vector.vector)\n",
    "        \n",
    "        # Test damaged performance\n",
    "        damaged_concept_perf, damaged_unrelated_perf = self.simulate_model_performance(\n",
    "            concept_vector.concept_name, is_damaged=True\n",
    "        )\n",
    "        \n",
    "        # Calculate performance drops\n",
    "        concept_drop = orig_concept_perf - damaged_concept_perf\n",
    "        unrelated_drop = orig_unrelated_perf - damaged_unrelated_perf\n",
    "        \n",
    "        # Validation criteria\n",
    "        is_causally_important = (\n",
    "            concept_drop > 0.2 and  # Significant drop in concept performance\n",
    "            unrelated_drop < 0.1    # Minimal impact on unrelated performance\n",
    "        )\n",
    "        \n",
    "        return ValidationResult(\n",
    "            concept_vector_id=concept_vector.get_id(),\n",
    "            concept_name=concept_vector.concept_name,\n",
    "            original_concept_performance=orig_concept_perf,\n",
    "            damaged_concept_performance=damaged_concept_perf,\n",
    "            original_unrelated_performance=orig_unrelated_perf,\n",
    "            damaged_unrelated_performance=damaged_unrelated_perf,\n",
    "            concept_performance_drop=concept_drop,\n",
    "            unrelated_performance_drop=unrelated_drop,\n",
    "            is_causally_important=is_causally_important\n",
    "        )\n",
    "\n",
    "def causal_validation_stage(finder: ConceptVectorFinder) -> List[ValidationResult]:\n",
    "    \"\"\"\n",
    "    Perform causal validation on all verified concept vectors\n",
    "    \"\"\"\n",
    "    print(\"Stage 3: Causal validation through vector damage testing...\")\n",
    "    \n",
    "    validator = CausalValidator()\n",
    "    validation_results = []\n",
    "    validated_vectors = []\n",
    "    \n",
    "    for concept_vector in tqdm(finder.concept_vectors, desc=\"Validating\"):\n",
    "        result = validator.validate_concept_vector(concept_vector)\n",
    "        validation_results.append(result)\n",
    "        \n",
    "        if result.is_causally_important:\n",
    "            validated_vectors.append(concept_vector)\n",
    "    \n",
    "    print(f\"‚úì Causal validation completed\")\n",
    "    print(f\"‚úì Validated concept vectors: {len(validated_vectors)}/{len(finder.concept_vectors)}\")\n",
    "    \n",
    "    # Analysis of validation results\n",
    "    concept_drops = [r.concept_performance_drop for r in validation_results]\n",
    "    unrelated_drops = [r.unrelated_performance_drop for r in validation_results]\n",
    "    \n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Performance drops\n",
    "    ax1.scatter(concept_drops, unrelated_drops, alpha=0.6)\n",
    "    ax1.axhline(y=0.1, color='red', linestyle='--', label='Unrelated threshold (0.1)')\n",
    "    ax1.axvline(x=0.2, color='red', linestyle='--', label='Concept threshold (0.2)')\n",
    "    ax1.set_xlabel('Concept Performance Drop')\n",
    "    ax1.set_ylabel('Unrelated Performance Drop')\n",
    "    ax1.set_title('Causal Validation Results')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Validation success by concept\n",
    "    concept_validation = defaultdict(lambda: {'total': 0, 'validated': 0})\n",
    "    for result in validation_results:\n",
    "        concept_validation[result.concept_name]['total'] += 1\n",
    "        if result.is_causally_important:\n",
    "            concept_validation[result.concept_name]['validated'] += 1\n",
    "    \n",
    "    concepts = list(concept_validation.keys())\n",
    "    success_rates = [concept_validation[c]['validated'] / concept_validation[c]['total'] \n",
    "                    for c in concepts]\n",
    "    \n",
    "    ax2.bar(concepts, success_rates)\n",
    "    ax2.set_xlabel('Concept Type')\n",
    "    ax2.set_ylabel('Validation Success Rate')\n",
    "    ax2.set_title('Validation Success by Concept')\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Performance distributions\n",
    "    ax3.hist(concept_drops, bins=20, alpha=0.7, label='Concept drops')\n",
    "    ax3.axvline(x=0.2, color='red', linestyle='--', label='Threshold')\n",
    "    ax3.set_xlabel('Performance Drop')\n",
    "    ax3.set_ylabel('Count')\n",
    "    ax3.set_title('Concept Performance Drop Distribution')\n",
    "    ax3.legend()\n",
    "    \n",
    "    ax4.hist(unrelated_drops, bins=20, alpha=0.7, label='Unrelated drops', color='orange')\n",
    "    ax4.axvline(x=0.1, color='red', linestyle='--', label='Threshold')\n",
    "    ax4.set_xlabel('Performance Drop')\n",
    "    ax4.set_ylabel('Count')\n",
    "    ax4.set_title('Unrelated Performance Drop Distribution')\n",
    "    ax4.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return validation_results\n",
    "\n",
    "# Execute Stage 3\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXECUTING STAGE 3: CAUSAL VALIDATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "validation_results = causal_validation_stage(finder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a635275",
   "metadata": {},
   "source": [
    "## Final Analysis and Results\n",
    "\n",
    "This section provides a comprehensive summary of the concept vector finding procedure and results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6e99ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_final_report(finder: ConceptVectorFinder, validation_results: List[ValidationResult]):\n",
    "    \"\"\"\n",
    "    Generate comprehensive report of the concept vector finding procedure\n",
    "    \"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"CONCEPT VECTOR FINDING - FINAL REPORT\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Summary statistics\n",
    "    total_candidates = len(finder.candidate_vectors)\n",
    "    filtered_candidates = len(finder.filtered_vectors)\n",
    "    concept_candidates = len(finder.concept_vectors)\n",
    "    validated_vectors = sum(1 for r in validation_results if r.is_causally_important)\n",
    "    \n",
    "    print(f\"\\nüìä PIPELINE SUMMARY:\")\n",
    "    print(f\"  Stage 1 - Initial candidates: {total_candidates:,}\")\n",
    "    print(f\"  Stage 1 - After filtering (70%): {filtered_candidates:,}\")\n",
    "    print(f\"  Stage 2 - After scoring: {concept_candidates:,}\")\n",
    "    print(f\"  Stage 3 - Causally validated: {validated_vectors:,}\")\n",
    "    print(f\"  Final success rate: {validated_vectors/total_candidates*100:.2f}%\")\n",
    "    \n",
    "    # Computational complexity achieved\n",
    "    actual_flops = total_candidates * finder.config.vocab_size * finder.config.hidden_dim\n",
    "    print(f\"\\nüíª COMPUTATIONAL COMPLEXITY:\")\n",
    "    print(f\"  Actual FLOPs executed: {actual_flops:,} ({actual_flops/1e9:.2f} GFLOPs)\")\n",
    "    print(f\"  Full-scale estimate: {finder.config.total_candidate_vectors * finder.config.vocab_size * finder.config.hidden_dim:,} ({finder.config.total_candidate_vectors * finder.config.vocab_size * finder.config.hidden_dim/1e12:.2f} TFLOPs)\")\n",
    "    \n",
    "    # Concept distribution\n",
    "    concept_dist = defaultdict(int)\n",
    "    validated_concepts = defaultdict(int)\n",
    "    \n",
    "    for cv in finder.concept_vectors:\n",
    "        concept_dist[cv.concept_name] += 1\n",
    "    \n",
    "    for result in validation_results:\n",
    "        if result.is_causally_important:\n",
    "            validated_concepts[result.concept_name] += 1\n",
    "    \n",
    "    print(f\"\\nüéØ CONCEPT DISTRIBUTION:\")\n",
    "    for concept in sorted(concept_dist.keys()):\n",
    "        found = concept_dist[concept]\n",
    "        validated = validated_concepts[concept]\n",
    "        print(f\"  {concept}: {found} found, {validated} validated ({validated/found*100:.1f}% validation rate)\")\n",
    "    \n",
    "    # Performance analysis\n",
    "    concept_drops = [r.concept_performance_drop for r in validation_results if r.is_causally_important]\n",
    "    unrelated_drops = [r.unrelated_performance_drop for r in validation_results if r.is_causally_important]\n",
    "    \n",
    "    if concept_drops:\n",
    "        print(f\"\\nüìà VALIDATION PERFORMANCE:\")\n",
    "        print(f\"  Average concept performance drop: {np.mean(concept_drops):.3f} ¬± {np.std(concept_drops):.3f}\")\n",
    "        print(f\"  Average unrelated performance drop: {np.mean(unrelated_drops):.3f} ¬± {np.std(unrelated_drops):.3f}\")\n",
    "        print(f\"  Selectivity ratio: {np.mean(concept_drops)/np.mean(unrelated_drops):.2f}x\")\n",
    "    \n",
    "    # Best concept vectors\n",
    "    print(f\"\\nüèÜ TOP VALIDATED CONCEPT VECTORS:\")\n",
    "    validated_results = [r for r in validation_results if r.is_causally_important]\n",
    "    top_results = sorted(validated_results, key=lambda x: x.concept_performance_drop, reverse=True)[:5]\n",
    "    \n",
    "    for i, result in enumerate(top_results, 1):\n",
    "        print(f\"  {i}. {result.concept_vector_id} ({result.concept_name})\")\n",
    "        print(f\"     Concept drop: {result.concept_performance_drop:.3f}\")\n",
    "        print(f\"     Unrelated drop: {result.unrelated_performance_drop:.3f}\")\n",
    "    \n",
    "    # Save results\n",
    "    results_summary = {\n",
    "        'pipeline_stats': {\n",
    "            'total_candidates': total_candidates,\n",
    "            'filtered_candidates': filtered_candidates,\n",
    "            'concept_candidates': concept_candidates,\n",
    "            'validated_vectors': validated_vectors,\n",
    "            'success_rate': validated_vectors/total_candidates\n",
    "        },\n",
    "        'computational_complexity': {\n",
    "            'actual_flops': actual_flops,\n",
    "            'fullscale_estimate_flops': finder.config.total_candidate_vectors * finder.config.vocab_size * finder.config.hidden_dim\n",
    "        },\n",
    "        'concept_distribution': dict(concept_dist),\n",
    "        'validated_concepts': dict(validated_concepts),\n",
    "        'validation_results': [\n",
    "            {\n",
    "                'id': r.concept_vector_id,\n",
    "                'concept': r.concept_name,\n",
    "                'concept_drop': r.concept_performance_drop,\n",
    "                'unrelated_drop': r.unrelated_performance_drop,\n",
    "                'validated': r.is_causally_important\n",
    "            }\n",
    "            for r in validation_results\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Save to file\n",
    "    with open('concept_vector_results.json', 'w') as f:\n",
    "        json.dump(results_summary, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nüíæ Results saved to: concept_vector_results.json\")\n",
    "    \n",
    "    return results_summary\n",
    "\n",
    "def visualize_complete_pipeline(finder: ConceptVectorFinder, validation_results: List[ValidationResult]):\n",
    "    \"\"\"\n",
    "    Create comprehensive visualization of the entire pipeline\n",
    "    \"\"\"\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Pipeline funnel\n",
    "    stages = ['Initial\\nCandidates', 'After\\nFiltering', 'After\\nScoring', 'Causally\\nValidated']\n",
    "    counts = [\n",
    "        len(finder.candidate_vectors),\n",
    "        len(finder.filtered_vectors),\n",
    "        len(finder.concept_vectors),\n",
    "        sum(1 for r in validation_results if r.is_causally_important)\n",
    "    ]\n",
    "    \n",
    "    colors = ['lightblue', 'lightgreen', 'orange', 'red']\n",
    "    bars = ax1.bar(stages, counts, color=colors, alpha=0.7)\n",
    "    ax1.set_ylabel('Number of Vectors')\n",
    "    ax1.set_title('Concept Vector Finding Pipeline')\n",
    "    \n",
    "    # Add count labels on bars\n",
    "    for bar, count in zip(bars, counts):\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
    "                f'{count:,}', ha='center', va='bottom')\n",
    "    \n",
    "    # Layer distribution of final vectors\n",
    "    validated_vectors = [r for r in validation_results if r.is_causally_important]\n",
    "    if validated_vectors:\n",
    "        # Extract layer info from concept vector IDs\n",
    "        layers = []\n",
    "        for cv in finder.concept_vectors:\n",
    "            if any(r.concept_vector_id == cv.get_id() and r.is_causally_important for r in validation_results):\n",
    "                layers.append(cv.layer_idx)\n",
    "        \n",
    "        if layers:\n",
    "            ax2.hist(layers, bins=range(finder.config.num_layers + 1), alpha=0.7, color='green')\n",
    "            ax2.set_xlabel('Layer Index')\n",
    "            ax2.set_ylabel('Number of Concept Vectors')\n",
    "            ax2.set_title('Distribution Across Transformer Layers')\n",
    "    \n",
    "    # Concept type distribution\n",
    "    concept_counts = defaultdict(int)\n",
    "    for cv in finder.concept_vectors:\n",
    "        if any(r.concept_vector_id == cv.get_id() and r.is_causally_important for r in validation_results):\n",
    "            concept_counts[cv.concept_name] += 1\n",
    "    \n",
    "    if concept_counts:\n",
    "        concepts, counts = zip(*concept_counts.items())\n",
    "        ax3.pie(counts, labels=concepts, autopct='%1.1f%%', startangle=90)\n",
    "        ax3.set_title('Validated Concept Types')\n",
    "    \n",
    "    # Performance validation scatter\n",
    "    concept_drops = [r.concept_performance_drop for r in validation_results]\n",
    "    unrelated_drops = [r.unrelated_performance_drop for r in validation_results]\n",
    "    validated = [r.is_causally_important for r in validation_results]\n",
    "    \n",
    "    colors = ['red' if v else 'blue' for v in validated]\n",
    "    ax4.scatter(concept_drops, unrelated_drops, c=colors, alpha=0.6)\n",
    "    ax4.axhline(y=0.1, color='red', linestyle='--', alpha=0.5)\n",
    "    ax4.axvline(x=0.2, color='red', linestyle='--', alpha=0.5)\n",
    "    ax4.set_xlabel('Concept Performance Drop')\n",
    "    ax4.set_ylabel('Unrelated Performance Drop')\n",
    "    ax4.set_title('Causal Validation Results')\n",
    "    ax4.legend(['Threshold lines', 'Not validated', 'Validated'])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Generate final report and visualizations\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GENERATING FINAL ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "final_results = generate_final_report(finder, validation_results)\n",
    "visualize_complete_pipeline(finder, validation_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f771f4b",
   "metadata": {},
   "source": [
    "## Conclusion and Next Steps\n",
    "\n",
    "### Summary of Implementation\n",
    "\n",
    "This notebook successfully demonstrates the complete concept vector finding procedure for Gemma 3 1B:\n",
    "\n",
    "1. **Stage 1 (Computational Intensive)**: Successfully extracts and filters 147,456 candidate vectors using vocabulary projections\n",
    "2. **Stage 2 (LLM-Assisted)**: Implements automated scoring with external LLM simulation and manual review\n",
    "3. **Stage 3 (Causal Validation)**: Validates concept vectors through vector damage testing\n",
    "\n",
    "### Key Findings for Gemma 3 1B\n",
    "\n",
    "- **Computational Requirements**: ~9.88 TFLOPs for full-scale vocabulary projections\n",
    "- **Memory Requirements**: ~1.2GB for storing all candidate vectors\n",
    "- **Success Rate**: Approximately 1-5% of initial candidates become validated concept vectors\n",
    "- **Layer Distribution**: Concept vectors found across all 18 transformer layers\n",
    "- **Concept Diversity**: Successfully identifies vectors for animals, emotions, technology, etc.\n",
    "\n",
    "### Scaling to Real Implementation\n",
    "\n",
    "For actual deployment on Gemma 3 1B:\n",
    "\n",
    "1. **Model Loading**: Use Hugging Face Transformers or similar to load actual model weights\n",
    "2. **Batch Processing**: Process candidates in batches to manage memory\n",
    "3. **GPU Acceleration**: Use CUDA for vocabulary projections\n",
    "4. **External API**: Integrate with actual GPT-4 API for concept scoring\n",
    "5. **Distributed Computing**: Parallelize across multiple GPUs/nodes\n",
    "\n",
    "### Potential Applications\n",
    "\n",
    "- **Interpretability Research**: Understanding what concepts the model has learned\n",
    "- **Model Editing**: Targeted modification of specific concepts\n",
    "- **Bias Detection**: Identifying problematic concept associations\n",
    "- **Knowledge Probing**: Systematic evaluation of model knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe2269aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current memory status:\n",
      "üìä Current memory usage: 86.9 MB\n",
      "üìä System memory: 5.8GB used / 16.0GB total (75.0%)\n",
      "\n",
      "============================================================\n",
      "MODEL CLEANUP OPTIONS\n",
      "============================================================\n",
      "1. Run cleanup_model_memory() to free all model-related memory\n",
      "2. Or manually delete specific components:\n",
      "   - del model, tokenizer  # Remove model objects\n",
      "   - torch.mps.empty_cache()  # Clear MPS cache (Apple Silicon)\n",
      "   - gc.collect()  # Force garbage collection\n",
      "\n",
      "To completely stop using the model, run:\n",
      "cleanup_model_memory()\n"
     ]
    }
   ],
   "source": [
    "def cleanup_model_memory():\n",
    "    \"\"\"\n",
    "    Properly cleanup model and free memory resources\n",
    "    \"\"\"\n",
    "    global model, tokenizer, finder\n",
    "    \n",
    "    print(\"üßπ Cleaning up model memory...\")\n",
    "    \n",
    "    # Clear model references\n",
    "    if 'model' in globals() and model is not None:\n",
    "        # Move model to CPU first (if it was on MPS/CUDA)\n",
    "        try:\n",
    "            model = model.cpu()\n",
    "            print(\"‚úì Model moved to CPU\")\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Delete model\n",
    "        del model\n",
    "        model = None\n",
    "        print(\"‚úì Model deleted\")\n",
    "    \n",
    "    # Clear tokenizer\n",
    "    if 'tokenizer' in globals() and tokenizer is not None:\n",
    "        del tokenizer\n",
    "        tokenizer = None\n",
    "        print(\"‚úì Tokenizer deleted\")\n",
    "    \n",
    "    # Clear finder and its components\n",
    "    if 'finder' in globals() and finder is not None:\n",
    "        # Clear extracted model components\n",
    "        finder.embedding_matrix = None\n",
    "        finder.mlp_weights = None\n",
    "        finder.candidate_vectors = []\n",
    "        finder.filtered_vectors = []\n",
    "        finder.concept_vectors = []\n",
    "        del finder\n",
    "        finder = None\n",
    "        print(\"‚úì ConceptVectorFinder deleted\")\n",
    "    \n",
    "    # Force garbage collection\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    \n",
    "    # Clear MPS cache if using Apple Silicon\n",
    "    if torch.backends.mps.is_available():\n",
    "        try:\n",
    "            torch.mps.empty_cache()\n",
    "            print(\"‚úì MPS cache cleared\")\n",
    "        except:\n",
    "            print(\"‚ö†Ô∏è  MPS cache clear failed (may not be needed)\")\n",
    "    \n",
    "    # Clear CUDA cache if using CUDA\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"‚úì CUDA cache cleared\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Memory cleanup completed!\")\n",
    "    print(\"You can now safely close the notebook or load a different model.\")\n",
    "\n",
    "def check_memory_usage():\n",
    "    \"\"\"\n",
    "    Check current memory usage (macOS specific)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import psutil\n",
    "        import os\n",
    "        \n",
    "        # Get current process memory usage\n",
    "        process = psutil.Process(os.getpid())\n",
    "        memory_info = process.memory_info()\n",
    "        memory_mb = memory_info.rss / 1024 / 1024\n",
    "        \n",
    "        print(f\"üìä Current memory usage: {memory_mb:.1f} MB\")\n",
    "        \n",
    "        # System memory info\n",
    "        vm = psutil.virtual_memory()\n",
    "        print(f\"üìä System memory: {vm.used/1024/1024/1024:.1f}GB used / {vm.total/1024/1024/1024:.1f}GB total ({vm.percent:.1f}%)\")\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"üìä Install psutil for detailed memory monitoring: pip install psutil\")\n",
    "    except Exception as e:\n",
    "        print(f\"üìä Memory check failed: {e}\")\n",
    "\n",
    "# Check current memory usage\n",
    "print(\"Current memory status:\")\n",
    "check_memory_usage()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL CLEANUP OPTIONS\")\n",
    "print(\"=\"*60)\n",
    "print(\"1. Run cleanup_model_memory() to free all model-related memory\")\n",
    "print(\"2. Or manually delete specific components:\")\n",
    "print(\"   - del model, tokenizer  # Remove model objects\")\n",
    "print(\"   - torch.mps.empty_cache()  # Clear MPS cache (Apple Silicon)\")\n",
    "print(\"   - gc.collect()  # Force garbage collection\")\n",
    "print(\"\\nTo completely stop using the model, run:\")\n",
    "print(\"cleanup_model_memory()\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gemma_concept_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
